<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>From Linear Regression to Deep Learning in Pytorch</title>
  <meta name="description" content="        From Linear Regression to Deep Learning in Pytorch    Linear Regression Let's implement a simple linear regression using torch    import torch    To ...">

  <link rel="canonical" href="https://alejandroschuler.github.io/CSL/linear_reg_torch.html">
  <link rel="alternate" type="application/rss+xml" title="Concepts in Supervised Learning" href="https://alejandroschuler.github.io/CSL/feed.xml">

  <meta property="og:url"         content="https://alejandroschuler.github.io/CSL/linear_reg_torch.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="From Linear Regression to Deep Learning in Pytorch" />
<meta property="og:description" content="        From Linear Regression to Deep Learning in Pytorch    Linear Regression Let's implement a simple linear regression using torch    import torch    To ..." />
<meta property="og:image"       content="" />

<meta name="twitter:card" content="summary">


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage": "https://alejandroschuler.github.io/CSL/linear_reg_torch.html",
  "headline": "From Linear Regression to Deep Learning in Pytorch",
  "datePublished": "2020-02-18T13:20:33-08:00",
  "dateModified": "2020-02-18T13:20:33-08:00",
  "description": "        From Linear Regression to Deep Learning in Pytorch    Linear Regression Let's implement a simple linear regression using torch    import torch    To ...",
  "author": {
    "@type": "Person",
    "name": "Alejandro Schuler"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://alejandroschuler.github.io/CSL",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://alejandroschuler.github.io/CSL",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/CSL/assets/css/styles.css">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/CSL/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<!-- (mostly) copied from nbconvert configuration -->
<!-- https://github.com/jupyter/nbconvert/blob/master/nbconvert/templates/html/mathjax.tpl -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true },
    },
    
    // Number LaTeX-style equations
    "TeX": {
        equationNumbers: {
          autoNumber: "all"
        }
    }
    
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML' async></script>


  <!-- DOM updating function -->
  <script src="/CSL/assets/js/page/dom-update.js"></script>

  <!-- Selectors for elements on the page -->
  <script src="/CSL/assets/js/page/documentSelectors.js"></script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/CSL';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js" async></script>
  <script src="/CSL/assets/js/page/anchors.js" async></script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/turbolinks/5.2.0/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->



  <!-- Load the auto-generating TOC (non-async otherwise the TOC won't load w/ turbolinks) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.8.1/tocbot.min.js" async></script>
  <script src="/CSL/assets/js/page/tocbot.js"></script>

  <!-- Google analytics -->
  


  <!-- Clipboard copy button -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script>

  <!-- Load custom website scripts -->
  <script src="/CSL/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/CSL/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/CSL/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  

  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.6/lunr.min.js" async></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>

  <!-- Load JS that depends on site variables -->
  <script src="/CSL/assets/js/page/copy-button.js" async></script>

  <!-- Hide cell code -->
  <script src="/CSL/assets/js/page/hide-cell.js" async></script>

  <!-- Printing the screen -->
  <!-- Include nbinteract for interactive widgets -->
<script src="https://printjs-4de6.kxcdn.com/print.min.js" async></script>
<script>
printContent = () => {
    // MathJax displays a second version of any math for assistive devices etc.
    // This prevents double-rendering in the PDF output.
    var ignoreAssistList = [];
    assistives = document.querySelectorAll('.MathJax_Display span.MJX_Assistive_MathML').forEach((element, index) => {
        var thisId = 'MathJax-assistive-' + index.toString();
        element.setAttribute('id', thisId);
        ignoreAssistList.push(thisId)
    });

    // Print the actual content object
    printJS({
        printable: 'textbook_content',
        type: 'html',
        css: "/CSL/assets/css/styles.css",
        style: "#textbook_content {padding-top: 40px};",
        scanStyles: false,
        targetStyles: ["*"],
        ignoreElements: ignoreAssistList,
        documentTitle: "Made with Jupyter Book"
    })
};

initPrint = () => {
    document.querySelector('#interact-button-print').addEventListener('click', printContent)
}

initFunction(initPrint)
</script>

</head>

  <body>
    <!-- Include the ThebeLab config so it gets reloaded on each page -->
    <script type="text/x-thebe-config">{
    requestKernel: true,
    binderOptions: {
    repo: "jupyter/jupyter-book",
    ref: "gh-pages",
    },
    codeMirrorConfig: {
    theme: "abcdef",
    mode: "python"
    },
    kernelOptions: {
    kernelName: "python3",
    path: "content"
    }
}
</script>

    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  
  <h2 class="c-sidebar__title">Concepts in Supervised Learning</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      
      
      

      
      
      <li class="c-sidebar__chapter" data-url="/linreg-dl-torch/intro">
        <a class="c-sidebar__entry"
          href="/CSL/linreg-dl-torch/intro.html"
        >
          
            1.
          
          From Linear Regression to Deep Learning in Pytorch
        </a>
      </li>

      
      

      

      
      

      
        

        

        <ul class="c-sidebar__sections u-hidden-visually">
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/linreg-dl-torch/linear-reg">
              <a class="c-sidebar__entry"
                href="/CSL/linreg-dl-torch/linear-reg.html"
              >
                
                  1.1
                
                Linear Regression
              </a>
            </li>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/linreg-dl-torch/logistic-reg">
              <a class="c-sidebar__entry"
                href="/CSL/linreg-dl-torch/logistic-reg.html"
              >
                
                  1.2
                
                Logistic Regression
              </a>
            </li>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/linreg-dl-torch/linear-reg-complex">
              <a class="c-sidebar__entry"
                href="/CSL/linreg-dl-torch/linear-reg-complex.html"
              >
                
                  1.3
                
                Linear-reg-complex
              </a>
            </li>
            
            
          
            
            

            
            
            
            

            <li class="c-sidebar__section" data-url="/linreg-dl-torch/abstracting-layers">
              <a class="c-sidebar__entry"
                href="/CSL/linreg-dl-torch/abstracting-layers.html"
              >
                
                  1.4
                
                Abstracting-layers
              </a>
            </li>
            
            
          
        </ul>
      

      
    
  </ul>
  <p class="sidebar_footer"></p>
</nav>

      
      <div class="c-topbar" id="top-navbar">
  <!-- We show the sidebar by default so we use .is-active -->
  <div class="c-topbar__buttons">
    <button
      id="js-sidebar-toggle"
      class="hamburger hamburger--arrowalt is-active"
    >
      <span class="hamburger-box">
        <span class="hamburger-inner"></span>
      </span>
    </button>
    <div class="buttons">
<div class="download-buttons-dropdown">
    <button id="dropdown-button-trigger" class="interact-button"><img src="/CSL/assets/images/download-solid.svg" alt="Download" /></button>
    <div class="download-buttons">
        <a href="/CSL/content/linear_reg_torch.ipynb" download>
        <button id="interact-button-download" class="interact-button">.ipynb</button>
        </a>
        
        <a id="interact-button-print"><button id="interact-button-download" class="interact-button">.pdf</button></a>
    </div>
</div>


  
  
  
  


</div>

  </div>
  <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
  <aside class="sidebar__right">
    <header><h4 class="nav__title"><img src="/CSL/assets/images/list-solid.svg" alt="Search" />   On this page</h4></header>
    <nav class="onthispage">
    </nav>
  </aside>
  <a href="/CSL/search.html" class="topbar-right-button" id="search-button">
    <img src="/CSL/assets/images/search-solid.svg" alt="Search" />
  </a>
</div>

      <main class="c-textbook__page" tabindex="-1">
            <div class="c-textbook__content" id="textbook_content">
                  <main class="jupyter-page">
    <div id="page-info"><div id="page-title">From Linear Regression to Deep Learning in Pytorch</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Linear-Regression">Linear Regression<a class="anchor-link" href="#Linear-Regression"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's implement a simple linear regression using torch</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To start with, we'll create some random data and store it as torch tensors.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (100 observations, 5 features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (100 observations)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Model">The Model<a class="anchor-link" href="#The-Model"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We're looking for parameters (coefficients) $\beta$ so that</p>
$$
\begin{array}{rcl}
y_i &amp; \approx &amp; \beta_0 + x_i\beta_{1:p}  \\
&amp;= &amp;\beta_0 + x_{i1}\beta_1 +x_{i2}\beta_2 \dots x_{ip}\beta_p
\end{array}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although you may not have seen it represented this way before, we can also write this model as a picture:</p>
<p><img src="images/linreg.png" width="250"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This will come in handy later when we get to more complex models, but for now you can think of the linear model in whatever way seems natural to you.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Loss">The Loss<a class="anchor-link" href="#The-Loss"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To quantify what we mean by a "good" approximation, we'll use the mean-squared-error loss. So for a given guess $\beta$ we'll give it the "grade"</p>
$$L(y,\hat y) = \frac{1}{n}\sum_i (y_i - \hat y_i)^2$$<p>where $\hat y_i  = x_i\beta_{1:p} + \beta_0$ and $n$ is the number of observations (rows) in the data. We're looking for the $\beta$ that gives us the best (lowest) grade. This combination of model (linear) and loss (mean-squared-error) is called linear regression.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Optimization-Algorithm">The Optimization Algorithm<a class="anchor-link" href="#The-Optimization-Algorithm"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are an infinite number of possible values for $\beta$ that we could try, so it would take us forever to try them all and see which gives the best loss. To get around this problem, we need some kind of optimization algorithm that is better than brute-force search. The algorithm we will use here is an extremely useful approach called gradient descent, which you should already be familiar with from our <a href="">previous exploration</a>.</p>
<p>To start, we'll initialize the coefficients $\beta$ with random numbers. That's our first guess. We'll update these random numbers using gradient descent to iteratively find better values that make the loss smaller.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">β</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 5 coefficients (one per feature) plus one intercept</span>
<span class="n">β</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="c1"># tell torch that β is going to have to save the gradient of something with respect to itself at some point</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.2731],
        [-0.1205],
        [-0.1281],
        [ 1.1380],
        [ 1.7139],
        [ 1.7325]], requires_grad=True)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Autograd">Autograd<a class="anchor-link" href="#Autograd"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Previously when using gradient descent we would have to analytically derive expressions for the gradient of the loss relative to each model parameter, implement these as functions in code, and call upon them at each gradient descent iteration. With pytorch, however, we can simply compute the current value of the loss and pytorch will automatically calculate all the necessary derivatives for us. Let's have a look.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># ŷ = xβ + β0</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="c1"># L = Σ(yᵢ-ŷᵢ)²/n</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(8.5604, grad_fn=&lt;DivBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute the gradients of the loss with respect to any tensors that went into the loss with requires_grad=true</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">β</span><span class="o">.</span><span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.4644],
        [-0.3877],
        [ 0.4751],
        [ 1.6801],
        [ 3.0645],
        [ 5.0251]])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>what we see here is a vector containing all of the derivatives we want. The first element is $\frac{\delta L}{\delta \beta_0}$, the second element is $\frac{\delta L}{\delta \beta_1}$, and so on. Note that this object is part of $\beta$ and not $L$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's calculate the gradient manually and make sure it matches up. We have:</p>
$$
\begin{array}{rcl}
\hat y_i &amp;=&amp; x_i\beta + \beta_0 \\
L(y,\hat y) &amp;=&amp; \frac{1}{n}\sum_i (y_i - \hat y_i)^2
\end{array}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So the derivative for $\beta_j$ with $j\ne 0$ is</p>
$$
\begin{array}{rcl}
\frac{\partial L}{\partial \beta_j} &amp;=&amp; 
\frac{1}{n}
\sum_i 
\frac{\partial L}{\partial y_i} 
\frac{\partial y_i}{\partial \beta_j}
\\
&amp;=&amp;
\frac{1}{n}
\sum_i
-2(y_i-\hat y_i)
x_{ij}
\\
&amp;=&amp;
-\frac{2}{n}
x_j^T(y-\hat y)
\end{array}
$$<p>and for $\beta_0$ is 
$$
\begin{array}{rcl}
\frac{\partial L}{\partial \beta_0}
&amp;=&amp;
\frac{1}{n}
\sum_i
-2(y_i-\hat y_i) \\
&amp;=&amp;
-\frac{2}{n}
1^T(y-\hat y)
\end{array}
$$</p>
<p>which means we can calculate the whole gradient as $-\frac{2}{n}[1,x]^T(y-[1,x]\beta)$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">beta_grad</span><span class="p">(</span><span class="n">β</span><span class="p">):</span>
    <span class="n">x_with_1s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">x</span><span class="p">)))</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_with_1s</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_with_1s</span><span class="p">,</span><span class="n">β</span><span class="p">))</span><span class="o">/</span><span class="n">n</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">beta_grad</span><span class="p">(</span><span class="n">β</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.4644],
        [-0.3877],
        [ 0.4751],
        [ 1.6801],
        [ 3.0645],
        [ 5.0251]], grad_fn=&lt;DivBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>which is exactly the same as the result in <code>β.grad</code>!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How does pytorch do this? It turns out that every pytorch tensor records not only its own value, but also what functions were called to produce it and which tensors went into those functions. The <code>torchvis</code> package lets us see this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchviz</span> <span class="k">import</span> <span class="n">make_dot</span>
<span class="n">make_dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;β&#39;</span><span class="p">:</span><span class="n">β</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="204pt" height="432pt"
 viewBox="0.00 0.00 204.31 432.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 428)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-428 200.31,-428 200.31,4 -4,4"/>
<!-- 4928321744 -->
<g id="node1" class="node">
<title>4928321744</title>
<polygon fill="#caff70" stroke="black" points="139.47,-20 51.18,-20 51.18,0 139.47,0 139.47,-20"/>
<text text-anchor="middle" x="95.32" y="-6.4" font-family="Times,serif" font-size="12.00">DivBackward0</text>
</g>
<!-- 4928321424 -->
<g id="node2" class="node">
<title>4928321424</title>
<polygon fill="lightgrey" stroke="black" points="141.48,-76 49.17,-76 49.17,-56 141.48,-56 141.48,-76"/>
<text text-anchor="middle" x="95.32" y="-62.4" font-family="Times,serif" font-size="12.00">SumBackward0</text>
</g>
<!-- 4928321424&#45;&gt;4928321744 -->
<g id="edge1" class="edge">
<title>4928321424&#45;&gt;4928321744</title>
<path fill="none" stroke="black" d="M95.32,-55.59C95.32,-48.7 95.32,-39.1 95.32,-30.57"/>
<polygon fill="black" stroke="black" points="98.82,-30.3 95.32,-20.3 91.82,-30.3 98.82,-30.3"/>
</g>
<!-- 4928321168 -->
<g id="node3" class="node">
<title>4928321168</title>
<polygon fill="lightgrey" stroke="black" points="141.31,-132 49.34,-132 49.34,-112 141.31,-112 141.31,-132"/>
<text text-anchor="middle" x="95.32" y="-118.4" font-family="Times,serif" font-size="12.00">PowBackward0</text>
</g>
<!-- 4928321168&#45;&gt;4928321424 -->
<g id="edge2" class="edge">
<title>4928321168&#45;&gt;4928321424</title>
<path fill="none" stroke="black" d="M95.32,-111.59C95.32,-104.7 95.32,-95.1 95.32,-86.57"/>
<polygon fill="black" stroke="black" points="98.82,-86.3 95.32,-76.3 91.82,-86.3 98.82,-86.3"/>
</g>
<!-- 4928320272 -->
<g id="node4" class="node">
<title>4928320272</title>
<polygon fill="lightgrey" stroke="black" points="140.14,-188 50.51,-188 50.51,-168 140.14,-168 140.14,-188"/>
<text text-anchor="middle" x="95.32" y="-174.4" font-family="Times,serif" font-size="12.00">SubBackward0</text>
</g>
<!-- 4928320272&#45;&gt;4928321168 -->
<g id="edge3" class="edge">
<title>4928320272&#45;&gt;4928321168</title>
<path fill="none" stroke="black" d="M95.32,-167.59C95.32,-160.7 95.32,-151.1 95.32,-142.57"/>
<polygon fill="black" stroke="black" points="98.82,-142.3 95.32,-132.3 91.82,-142.3 98.82,-142.3"/>
</g>
<!-- 4928321296 -->
<g id="node5" class="node">
<title>4928321296</title>
<polygon fill="lightgrey" stroke="black" points="141.14,-244 49.51,-244 49.51,-224 141.14,-224 141.14,-244"/>
<text text-anchor="middle" x="95.32" y="-230.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 4928321296&#45;&gt;4928320272 -->
<g id="edge4" class="edge">
<title>4928321296&#45;&gt;4928320272</title>
<path fill="none" stroke="black" d="M95.32,-223.59C95.32,-216.7 95.32,-207.1 95.32,-198.57"/>
<polygon fill="black" stroke="black" points="98.82,-198.3 95.32,-188.3 91.82,-198.3 98.82,-198.3"/>
</g>
<!-- 4928319888 -->
<g id="node6" class="node">
<title>4928319888</title>
<polygon fill="lightgrey" stroke="black" points="84.47,-300 0.18,-300 0.18,-280 84.47,-280 84.47,-300"/>
<text text-anchor="middle" x="42.32" y="-286.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 4928319888&#45;&gt;4928321296 -->
<g id="edge5" class="edge">
<title>4928319888&#45;&gt;4928321296</title>
<path fill="none" stroke="black" d="M51.56,-279.59C59.16,-271.85 70.11,-260.69 79.16,-251.47"/>
<polygon fill="black" stroke="black" points="81.69,-253.89 86.2,-244.3 76.7,-248.98 81.69,-253.89"/>
</g>
<!-- 4928323472 -->
<g id="node7" class="node">
<title>4928323472</title>
<polygon fill="lightgrey" stroke="black" points="103.46,-356 15.18,-356 15.18,-336 103.46,-336 103.46,-356"/>
<text text-anchor="middle" x="59.32" y="-342.4" font-family="Times,serif" font-size="12.00">SliceBackward</text>
</g>
<!-- 4928323472&#45;&gt;4928319888 -->
<g id="edge6" class="edge">
<title>4928323472&#45;&gt;4928319888</title>
<path fill="none" stroke="black" d="M56.36,-335.59C54.14,-328.55 51.04,-318.67 48.31,-310"/>
<polygon fill="black" stroke="black" points="51.59,-308.79 45.25,-300.3 44.92,-310.89 51.59,-308.79"/>
</g>
<!-- 4928322064 -->
<g id="node8" class="node">
<title>4928322064</title>
<polygon fill="lightblue" stroke="black" points="122.32,-424 68.32,-424 68.32,-392 122.32,-392 122.32,-424"/>
<text text-anchor="middle" x="95.32" y="-410.4" font-family="Times,serif" font-size="12.00">β</text>
<text text-anchor="middle" x="95.32" y="-398.4" font-family="Times,serif" font-size="12.00"> (6, 1)</text>
</g>
<!-- 4928322064&#45;&gt;4928323472 -->
<g id="edge7" class="edge">
<title>4928322064&#45;&gt;4928323472</title>
<path fill="none" stroke="black" d="M86.24,-391.86C81.34,-383.69 75.25,-373.55 70.11,-364.98"/>
<polygon fill="black" stroke="black" points="72.96,-362.93 64.82,-356.15 66.96,-366.53 72.96,-362.93"/>
</g>
<!-- 4928320720 -->
<g id="node9" class="node">
<title>4928320720</title>
<polygon fill="lightgrey" stroke="black" points="196.29,-300 102.36,-300 102.36,-280 196.29,-280 196.29,-300"/>
<text text-anchor="middle" x="149.32" y="-286.4" font-family="Times,serif" font-size="12.00">SelectBackward</text>
</g>
<!-- 4928322064&#45;&gt;4928320720 -->
<g id="edge9" class="edge">
<title>4928322064&#45;&gt;4928320720</title>
<path fill="none" stroke="black" d="M102.38,-391.84C112.25,-370.64 130.15,-332.2 140.76,-309.39"/>
<polygon fill="black" stroke="black" points="144.02,-310.68 145.07,-300.14 137.68,-307.72 144.02,-310.68"/>
</g>
<!-- 4928320720&#45;&gt;4928321296 -->
<g id="edge8" class="edge">
<title>4928320720&#45;&gt;4928321296</title>
<path fill="none" stroke="black" d="M139.92,-279.59C132.17,-271.85 121.01,-260.69 111.79,-251.47"/>
<polygon fill="black" stroke="black" points="114.17,-248.89 104.62,-244.3 109.22,-253.84 114.17,-248.89"/>
</g>
</g>
</svg>

</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To calculate the gradient of $L$ with respect to $\beta$, pytorch takes the gradients of each of these functions in turn (which are simple and built-in to pytorch), evaluates them at their current value using the input tensors, and multiplies them together to arrive at the answer you would get via the chain rule. You can learn more about this process <a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95">here</a> and elsewhere.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Doing-Gradient-Descent">Doing Gradient Descent<a class="anchor-link" href="#Doing-Gradient-Descent"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To update the weights, we need to subtract the gradient (times a small learning rate) from the current value of the weights.</p>
<p>We do this from inside a <code>no_grad():</code> "context" so that $\beta$ doesn't store the history of the update (try the update without the <code>with torch.no_grad():</code> and see what happens). We also clear out <code>β.grad</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">β</span> <span class="o">-=</span> <span class="mf">10e-5</span> <span class="o">*</span> <span class="n">β</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># β = β - 10e-5 * β.grad</span>
    <span class="n">β</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">β</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.2731],
        [-0.1204],
        [-0.1282],
        [ 1.1378],
        [ 1.7136],
        [ 1.7320]], requires_grad=True)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, our new value of $\beta$ is slightly different than what we started with because we've taken a single gradient step.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>ZEROING GRADIENTS</strong></p>
<p>If we don't zero the gradient, the next time we calculate the gradient of something with respect to $\beta$, the new gradient will be added to whatever was stored there instead of overwriting it. That's just the way torch was made to work.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE</strong></p>
<p>Investigate for yourself (either by testing code or googling) why the parameter update and gradient zerioing should be performed within a <code>torch.no_grad()</code> context.</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Looping">Looping<a class="anchor-link" href="#Looping"> </a></h4>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's repeat what we have so far but now add a little loop to train our model for 500 gradient descent iterations instead of going slowly though a single iteration:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (10 observations, 5 features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (10 observations)</span>

<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># to kep track of the loss over the iterations</span>
<span class="n">β</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 5 coefficients (one per feature) plus one intercept</span>
<span class="n">β</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="c1"># tell torch that β is going to have to save the gradient of something with respect to itself at some point</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.1092],
        [-1.3379],
        [ 0.2449],
        [-0.8324],
        [-0.3387],
        [ 0.4786]], requires_grad=True)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># ŷ = xβ + β0 (calculate predictions)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="c1"># L = Σ(yᵢ-ŷᵢ)²/n (use predictions to calculate loss)</span>
    
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients (in this case δL/δβ, δL/δW)</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># take the gradient descent step </span>
        <span class="n">β</span> <span class="o">-=</span> <span class="mf">10e-3</span> <span class="o">*</span> <span class="n">β</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">β</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can see how the loss changes over the iterations:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_42_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We see training loss goes down.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And here's the value of $\beta$ after 500 iterations:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">β</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.0952],
        [-0.1218],
        [-0.1502],
        [-0.1120],
        [-0.0219],
        [ 0.1727]], requires_grad=True)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Remember our original data is totally random and there is no relationship between the predictors and outcomes. So the "right" answer for what $\beta$ should be in this case is $\beta=0$. As we see above, all the values are near 0, so our algorithm appears to be converging to the right answer.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To predict for a new observation, all we have to do is multiply by $\beta$ and add the intercept:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 10 new observations</span>
<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># predictions</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.1722],
        [ 0.3556],
        [-0.0930],
        [-0.0107],
        [-0.1134],
        [ 0.1378],
        [ 0.4761],
        [-0.3901],
        [ 0.2622],
        [-0.6157]], grad_fn=&lt;AddBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We just developed a machine learning method out of these three components:</p>
<ul>
<li>linear model (model)</li>
<li>MSE loss (loss)</li>
<li>gradient descent (search algorithm)</li>
</ul>
<p>These components are like interchangable parts. We're going to see how we can use a different model and loss to fit data of a different kind without fundamentally changing our search strategy.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Logistic-Regression">Logistic Regression<a class="anchor-link" href="#Logistic-Regression"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we've seen how this works for linear regression, we can try and implement a model that predicts probabilities for a two-class outcome instead of a prediction for a continuous outcome. Now our data look like this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span><span class="mi">5</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># predictors (10 observations, 5 features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># outcomes (10 observations)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0., 0., 1., 0., 1.])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Changing-the-model">Changing the model<a class="anchor-link" href="#Changing-the-model"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To do this, we have to change two things: firstly, our model can't be $\hat y_i  = x_i\beta_{1:p} + \beta_0$ anymore because $\hat y_i$ will come out as a number between plus and minus infinity, wheras we now want it to be a number between 0 and 1 (a probability). To fix this, we'll apply a function that takes $x_i\beta_{1:p} + \beta_0$ and squishes it down to a number between 0 and 1. The function we'll use is $\sigma(z) = \frac{1}{1+e^{-z}}$, which is called the <em>sigmoid</em> or <em>logistic</em> function.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So our model will be $\hat p_i  = \sigma(x_i\beta_{1:p} + \beta_0)$. The parameters are still $\beta$, but now there is an additional (fixed) squishing function that makes sure the output is a probability. In accordance, I've called the output $\hat p_i$ instead of $\hat y_i$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As before, we can represent this model graphically:</p>
<p><img src="images/logreg.png" width="250"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the only difference between this picture and the one before is that we've inserted $\sigma()$ right before the final output.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Changing-the-loss">Changing the loss<a class="anchor-link" href="#Changing-the-loss"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our loss function from before was MSE: $\sum (\hat y_i - y_i)^2$, which compares a predicted outcome to the true outcome. One option is to replace the predicted outcome with a predicted probability to get $\sum (\hat p_i - y_i)^2$ (the <em>Brier Score</em>). Here's what that looks like (for a single observation $i$):</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="http://journal.sjdm.org/16/16218/jdm16218001.png" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is ok, but many people don't like that there is a maximum penalty under this loss: if the true outcome was $y_i =0$ but the model predicts $p_j = P(Y_i=1) = 1$ (i.e. $y_i =1$ with absolute certainty), then the penalty should be infinitely high to strongly discrourage this kind of overconfident (wrong) prediction. Under the Brier score, however, you can see that this kind of prediction only incurrs a loss of 1, not infinity. Furthermore, when the outcome is 1 and the prediction gets closer and closer to 1, we see that the loss "flattens out", which means that we're not encouraged to be more and more confident when we're right.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What we want is a loss function that looks like this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://conorsdatablog.files.wordpress.com/2018/03/log_loss.png?w=615&amp;h=597" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is called the <em>log-loss</em> (or <em>cross-entropy loss</em> or <em>binomial deviance</em>). It is expressed as</p>
$$
L(\hat p, y) = 
\frac{1}{n}
\sum_i 
\begin{cases}
-\log(P(Y_i = 1)) &amp; \text{if}\ y_i=1 \\
-\log(P(Y_i = 0)) &amp; \text{if}\ y_i=0
\end{cases}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>which is the same as</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
=
\frac{1}{n}
\sum_i 
\begin{cases}
-\log(\hat p_i) &amp; \text{if}\ y_i=1 \\
-\log(1-\hat p_i) &amp; \text{if}\ y_i=0
\end{cases}
\\
= 
-\frac{1}{n}
\sum_i 
y_i \log(\hat p_i) + (1-y_i) \log (1-\hat p_i)
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Search-strategy-is-still-gradient-descent">Search strategy is still gradient descent<a class="anchor-link" href="#Search-strategy-is-still-gradient-descent"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This combination of logistic-linear model and log-loss is called <em>logistic regression</em> (although this is a misnomer because we're actually doing classification, not regression). The amazing thing about gradient descent is that, as long as you can differentiate through the loss and to the model parameters, it just works. Compare this code to what we did to fit our linear regression model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (10 observations, 5 features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># outcomes (10 observations)</span>

<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># to kep track of the loss over the iterations</span>
<span class="n">β</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 5 coefficients (one per feature)</span>
<span class="n">β</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="c1"># tell torch that β is going to have to save the gradient of something with respect to itself at some point</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-1.6644],
        [-1.8794],
        [ 1.4279],
        [-1.1967],
        [-0.1365],
        [ 0.8001]], requires_grad=True)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
    <span class="n">p̂</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    <span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p̂</span><span class="p">)</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p̂</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">n</span> <span class="c1"># log loss</span>
    
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients (in this case δL/δβ, δL/δW)</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># take the gradient descent step </span>
        <span class="n">β</span> <span class="o">-=</span> <span class="mf">10e-4</span> <span class="o">*</span> <span class="n">β</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">β</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_74_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although the code looks pretty much the same, save the changes to the model and loss, the computation that pytorch needs to do to compute the gradient of $L$ with respect to each $\beta$ is now much more involved, which you too will see if you try to compute it by hand.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">make_dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;β&#39;</span><span class="p">:</span><span class="n">β</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="209pt" height="824pt"
 viewBox="0.00 0.00 208.98 824.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 820)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-820 204.98,-820 204.98,4 -4,4"/>
<!-- 4928378832 -->
<g id="node1" class="node">
<title>4928378832</title>
<polygon fill="#caff70" stroke="black" points="143.47,-20 55.18,-20 55.18,0 143.47,0 143.47,-20"/>
<text text-anchor="middle" x="99.32" y="-6.4" font-family="Times,serif" font-size="12.00">DivBackward0</text>
</g>
<!-- 4928377936 -->
<g id="node2" class="node">
<title>4928377936</title>
<polygon fill="lightgrey" stroke="black" points="141.46,-76 57.19,-76 57.19,-56 141.46,-56 141.46,-76"/>
<text text-anchor="middle" x="99.32" y="-62.4" font-family="Times,serif" font-size="12.00">NegBackward</text>
</g>
<!-- 4928377936&#45;&gt;4928378832 -->
<g id="edge1" class="edge">
<title>4928377936&#45;&gt;4928378832</title>
<path fill="none" stroke="black" d="M99.32,-55.59C99.32,-48.7 99.32,-39.1 99.32,-30.57"/>
<polygon fill="black" stroke="black" points="102.82,-30.3 99.32,-20.3 95.82,-30.3 102.82,-30.3"/>
</g>
<!-- 4927980624 -->
<g id="node3" class="node">
<title>4927980624</title>
<polygon fill="lightgrey" stroke="black" points="145.48,-132 53.17,-132 53.17,-112 145.48,-112 145.48,-132"/>
<text text-anchor="middle" x="99.32" y="-118.4" font-family="Times,serif" font-size="12.00">SumBackward0</text>
</g>
<!-- 4927980624&#45;&gt;4928377936 -->
<g id="edge2" class="edge">
<title>4927980624&#45;&gt;4928377936</title>
<path fill="none" stroke="black" d="M99.32,-111.59C99.32,-104.7 99.32,-95.1 99.32,-86.57"/>
<polygon fill="black" stroke="black" points="102.82,-86.3 99.32,-76.3 95.82,-86.3 102.82,-86.3"/>
</g>
<!-- 4927979792 -->
<g id="node4" class="node">
<title>4927979792</title>
<polygon fill="lightgrey" stroke="black" points="145.14,-188 53.51,-188 53.51,-168 145.14,-168 145.14,-188"/>
<text text-anchor="middle" x="99.32" y="-174.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 4927979792&#45;&gt;4927980624 -->
<g id="edge3" class="edge">
<title>4927979792&#45;&gt;4927980624</title>
<path fill="none" stroke="black" d="M99.32,-167.59C99.32,-160.7 99.32,-151.1 99.32,-142.57"/>
<polygon fill="black" stroke="black" points="102.82,-142.3 99.32,-132.3 95.82,-142.3 102.82,-142.3"/>
</g>
<!-- 4927980112 -->
<g id="node5" class="node">
<title>4927980112</title>
<polygon fill="lightgrey" stroke="black" points="90.47,-244 0.18,-244 0.18,-224 90.47,-224 90.47,-244"/>
<text text-anchor="middle" x="45.32" y="-230.4" font-family="Times,serif" font-size="12.00">MulBackward0</text>
</g>
<!-- 4927980112&#45;&gt;4927979792 -->
<g id="edge4" class="edge">
<title>4927980112&#45;&gt;4927979792</title>
<path fill="none" stroke="black" d="M54.73,-223.59C62.47,-215.85 73.64,-204.69 82.86,-195.47"/>
<polygon fill="black" stroke="black" points="85.43,-197.84 90.03,-188.3 80.48,-192.89 85.43,-197.84"/>
</g>
<!-- 4927982800 -->
<g id="node6" class="node">
<title>4927982800</title>
<polygon fill="lightgrey" stroke="black" points="87.3,-356 3.35,-356 3.35,-336 87.3,-336 87.3,-356"/>
<text text-anchor="middle" x="45.32" y="-342.4" font-family="Times,serif" font-size="12.00">LogBackward</text>
</g>
<!-- 4927982800&#45;&gt;4927980112 -->
<g id="edge5" class="edge">
<title>4927982800&#45;&gt;4927980112</title>
<path fill="none" stroke="black" d="M45.32,-336C45.32,-318.19 45.32,-278.12 45.32,-254.15"/>
<polygon fill="black" stroke="black" points="48.82,-254.13 45.32,-244.13 41.82,-254.13 48.82,-254.13"/>
</g>
<!-- 4927981840 -->
<g id="node7" class="node">
<title>4927981840</title>
<polygon fill="lightgrey" stroke="black" points="143.47,-412 55.18,-412 55.18,-392 143.47,-392 143.47,-412"/>
<text text-anchor="middle" x="99.32" y="-398.4" font-family="Times,serif" font-size="12.00">DivBackward0</text>
</g>
<!-- 4927981840&#45;&gt;4927982800 -->
<g id="edge6" class="edge">
<title>4927981840&#45;&gt;4927982800</title>
<path fill="none" stroke="black" d="M89.92,-391.59C82.17,-383.85 71.01,-372.69 61.79,-363.47"/>
<polygon fill="black" stroke="black" points="64.17,-360.89 54.62,-356.3 59.22,-365.84 64.17,-360.89"/>
</g>
<!-- 4928161616 -->
<g id="node20" class="node">
<title>4928161616</title>
<polygon fill="lightgrey" stroke="black" points="201.14,-356 105.51,-356 105.51,-336 201.14,-336 201.14,-356"/>
<text text-anchor="middle" x="153.32" y="-342.4" font-family="Times,serif" font-size="12.00">RsubBackward1</text>
</g>
<!-- 4927981840&#45;&gt;4928161616 -->
<g id="edge22" class="edge">
<title>4927981840&#45;&gt;4928161616</title>
<path fill="none" stroke="black" d="M108.73,-391.59C116.47,-383.85 127.64,-372.69 136.86,-363.47"/>
<polygon fill="black" stroke="black" points="139.43,-365.84 144.03,-356.3 134.48,-360.89 139.43,-365.84"/>
</g>
<!-- 4927980496 -->
<g id="node8" class="node">
<title>4927980496</title>
<polygon fill="lightgrey" stroke="black" points="88.3,-468 4.35,-468 4.35,-448 88.3,-448 88.3,-468"/>
<text text-anchor="middle" x="46.32" y="-454.4" font-family="Times,serif" font-size="12.00">ExpBackward</text>
</g>
<!-- 4927980496&#45;&gt;4927981840 -->
<g id="edge7" class="edge">
<title>4927980496&#45;&gt;4927981840</title>
<path fill="none" stroke="black" d="M55.56,-447.59C63.16,-439.85 74.11,-428.69 83.16,-419.47"/>
<polygon fill="black" stroke="black" points="85.69,-421.89 90.2,-412.3 80.7,-416.98 85.69,-421.89"/>
</g>
<!-- 4928160144 -->
<g id="node9" class="node">
<title>4928160144</title>
<polygon fill="lightgrey" stroke="black" points="90.46,-580 6.19,-580 6.19,-560 90.46,-560 90.46,-580"/>
<text text-anchor="middle" x="48.32" y="-566.4" font-family="Times,serif" font-size="12.00">NegBackward</text>
</g>
<!-- 4928160144&#45;&gt;4927980496 -->
<g id="edge8" class="edge">
<title>4928160144&#45;&gt;4927980496</title>
<path fill="none" stroke="black" d="M48.16,-560C47.84,-542.19 47.11,-502.12 46.67,-478.15"/>
<polygon fill="black" stroke="black" points="50.17,-478.06 46.49,-468.13 43.17,-478.19 50.17,-478.06"/>
</g>
<!-- 4928162896 -->
<g id="node10" class="node">
<title>4928162896</title>
<polygon fill="lightgrey" stroke="black" points="145.14,-636 53.51,-636 53.51,-616 145.14,-616 145.14,-636"/>
<text text-anchor="middle" x="99.32" y="-622.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 4928162896&#45;&gt;4928160144 -->
<g id="edge9" class="edge">
<title>4928162896&#45;&gt;4928160144</title>
<path fill="none" stroke="black" d="M90.44,-615.59C83.2,-607.93 72.8,-596.91 64.14,-587.75"/>
<polygon fill="black" stroke="black" points="66.52,-585.17 57.11,-580.3 61.43,-589.97 66.52,-585.17"/>
</g>
<!-- 4928162000 -->
<g id="node17" class="node">
<title>4928162000</title>
<polygon fill="lightgrey" stroke="black" points="193.46,-580 109.19,-580 109.19,-560 193.46,-560 193.46,-580"/>
<text text-anchor="middle" x="151.32" y="-566.4" font-family="Times,serif" font-size="12.00">NegBackward</text>
</g>
<!-- 4928162896&#45;&gt;4928162000 -->
<g id="edge18" class="edge">
<title>4928162896&#45;&gt;4928162000</title>
<path fill="none" stroke="black" d="M108.38,-615.59C115.76,-607.93 126.37,-596.91 135.2,-587.75"/>
<polygon fill="black" stroke="black" points="137.95,-589.93 142.37,-580.3 132.91,-585.07 137.95,-589.93"/>
</g>
<!-- 4928160336 -->
<g id="node11" class="node">
<title>4928160336</title>
<polygon fill="lightgrey" stroke="black" points="88.47,-692 4.18,-692 4.18,-672 88.47,-672 88.47,-692"/>
<text text-anchor="middle" x="46.32" y="-678.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 4928160336&#45;&gt;4928162896 -->
<g id="edge10" class="edge">
<title>4928160336&#45;&gt;4928162896</title>
<path fill="none" stroke="black" d="M55.56,-671.59C63.16,-663.85 74.11,-652.69 83.16,-643.47"/>
<polygon fill="black" stroke="black" points="85.69,-645.89 90.2,-636.3 80.7,-640.98 85.69,-645.89"/>
</g>
<!-- 4928162448 -->
<g id="node12" class="node">
<title>4928162448</title>
<polygon fill="lightgrey" stroke="black" points="107.46,-748 19.18,-748 19.18,-728 107.46,-728 107.46,-748"/>
<text text-anchor="middle" x="63.32" y="-734.4" font-family="Times,serif" font-size="12.00">SliceBackward</text>
</g>
<!-- 4928162448&#45;&gt;4928160336 -->
<g id="edge11" class="edge">
<title>4928162448&#45;&gt;4928160336</title>
<path fill="none" stroke="black" d="M60.36,-727.59C58.14,-720.55 55.04,-710.67 52.31,-702"/>
<polygon fill="black" stroke="black" points="55.59,-700.79 49.25,-692.3 48.92,-702.89 55.59,-700.79"/>
</g>
<!-- 4928161424 -->
<g id="node13" class="node">
<title>4928161424</title>
<polygon fill="lightblue" stroke="black" points="126.32,-816 72.32,-816 72.32,-784 126.32,-784 126.32,-816"/>
<text text-anchor="middle" x="99.32" y="-802.4" font-family="Times,serif" font-size="12.00">β</text>
<text text-anchor="middle" x="99.32" y="-790.4" font-family="Times,serif" font-size="12.00"> (6, 1)</text>
</g>
<!-- 4928161424&#45;&gt;4928162448 -->
<g id="edge12" class="edge">
<title>4928161424&#45;&gt;4928162448</title>
<path fill="none" stroke="black" d="M90.24,-783.86C85.34,-775.69 79.25,-765.55 74.11,-756.98"/>
<polygon fill="black" stroke="black" points="76.96,-754.93 68.82,-748.15 70.96,-758.53 76.96,-754.93"/>
</g>
<!-- 4928163600 -->
<g id="node14" class="node">
<title>4928163600</title>
<polygon fill="lightgrey" stroke="black" points="200.29,-692 106.36,-692 106.36,-672 200.29,-672 200.29,-692"/>
<text text-anchor="middle" x="153.32" y="-678.4" font-family="Times,serif" font-size="12.00">SelectBackward</text>
</g>
<!-- 4928161424&#45;&gt;4928163600 -->
<g id="edge14" class="edge">
<title>4928161424&#45;&gt;4928163600</title>
<path fill="none" stroke="black" d="M106.38,-783.84C116.25,-762.64 134.15,-724.2 144.76,-701.39"/>
<polygon fill="black" stroke="black" points="148.02,-702.68 149.07,-692.14 141.68,-699.72 148.02,-702.68"/>
</g>
<!-- 4928163600&#45;&gt;4928162896 -->
<g id="edge13" class="edge">
<title>4928163600&#45;&gt;4928162896</title>
<path fill="none" stroke="black" d="M143.92,-671.59C136.17,-663.85 125.01,-652.69 115.79,-643.47"/>
<polygon fill="black" stroke="black" points="118.17,-640.89 108.62,-636.3 113.22,-645.84 118.17,-640.89"/>
</g>
<!-- 4928193744 -->
<g id="node15" class="node">
<title>4928193744</title>
<polygon fill="lightgrey" stroke="black" points="198.14,-468 106.51,-468 106.51,-448 198.14,-448 198.14,-468"/>
<text text-anchor="middle" x="152.32" y="-454.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 4928193744&#45;&gt;4927981840 -->
<g id="edge15" class="edge">
<title>4928193744&#45;&gt;4927981840</title>
<path fill="none" stroke="black" d="M143.09,-447.59C135.49,-439.85 124.54,-428.69 115.49,-419.47"/>
<polygon fill="black" stroke="black" points="117.95,-416.98 108.45,-412.3 112.96,-421.89 117.95,-416.98"/>
</g>
<!-- 4928161168 -->
<g id="node16" class="node">
<title>4928161168</title>
<polygon fill="lightgrey" stroke="black" points="193.3,-524 109.35,-524 109.35,-504 193.3,-504 193.3,-524"/>
<text text-anchor="middle" x="151.32" y="-510.4" font-family="Times,serif" font-size="12.00">ExpBackward</text>
</g>
<!-- 4928161168&#45;&gt;4928193744 -->
<g id="edge16" class="edge">
<title>4928161168&#45;&gt;4928193744</title>
<path fill="none" stroke="black" d="M151.5,-503.59C151.63,-496.7 151.8,-487.1 151.96,-478.57"/>
<polygon fill="black" stroke="black" points="155.47,-478.36 152.15,-468.3 148.47,-478.23 155.47,-478.36"/>
</g>
<!-- 4928162000&#45;&gt;4928161168 -->
<g id="edge17" class="edge">
<title>4928162000&#45;&gt;4928161168</title>
<path fill="none" stroke="black" d="M151.32,-559.59C151.32,-552.7 151.32,-543.1 151.32,-534.57"/>
<polygon fill="black" stroke="black" points="154.82,-534.3 151.32,-524.3 147.82,-534.3 154.82,-534.3"/>
</g>
<!-- 4927980816 -->
<g id="node18" class="node">
<title>4927980816</title>
<polygon fill="lightgrey" stroke="black" points="199.47,-244 109.18,-244 109.18,-224 199.47,-224 199.47,-244"/>
<text text-anchor="middle" x="154.32" y="-230.4" font-family="Times,serif" font-size="12.00">MulBackward0</text>
</g>
<!-- 4927980816&#45;&gt;4927979792 -->
<g id="edge19" class="edge">
<title>4927980816&#45;&gt;4927979792</title>
<path fill="none" stroke="black" d="M144.74,-223.59C136.86,-215.85 125.49,-204.69 116.1,-195.47"/>
<polygon fill="black" stroke="black" points="118.38,-192.81 108.79,-188.3 113.48,-197.8 118.38,-192.81"/>
</g>
<!-- 4927981072 -->
<g id="node19" class="node">
<title>4927981072</title>
<polygon fill="lightgrey" stroke="black" points="195.3,-300 111.35,-300 111.35,-280 195.3,-280 195.3,-300"/>
<text text-anchor="middle" x="153.32" y="-286.4" font-family="Times,serif" font-size="12.00">LogBackward</text>
</g>
<!-- 4927981072&#45;&gt;4927980816 -->
<g id="edge20" class="edge">
<title>4927981072&#45;&gt;4927980816</title>
<path fill="none" stroke="black" d="M153.5,-279.59C153.63,-272.7 153.8,-263.1 153.96,-254.57"/>
<polygon fill="black" stroke="black" points="157.47,-254.36 154.15,-244.3 150.47,-254.23 157.47,-254.36"/>
</g>
<!-- 4928161616&#45;&gt;4927981072 -->
<g id="edge21" class="edge">
<title>4928161616&#45;&gt;4927981072</title>
<path fill="none" stroke="black" d="M153.32,-335.59C153.32,-328.7 153.32,-319.1 153.32,-310.57"/>
<polygon fill="black" stroke="black" points="156.82,-310.3 153.32,-300.3 149.82,-310.3 156.82,-310.3"/>
</g>
</g>
</svg>

</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The amazing thing is that pytorch just handles it and you don't have to.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To predict for a new observation, all we have to do is multiply by $\beta_{1:p}$, add the intercept $\beta_0$, and pass through $\sigma$:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 10 new observations</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="c1"># probability predictions</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.4700],
        [0.4699],
        [0.4700],
        [0.4701],
        [0.4700],
        [0.4700],
        [0.4700],
        [0.4701],
        [0.4700],
        [0.4700]], grad_fn=&lt;DivBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE</strong></p>
<p>Why don't we use <em>accuracy</em> as a loss? In other words,</p>
$$
L(y, \hat p) = \sum_i
\begin{cases}
1 &amp; \text{if}\ y_i = 1 \ \text{and} \ \hat p_i &gt; 0.5 \\
1 &amp; \text{if}\ y_i = 0 \ \text{and} \ \hat p_i \le 0.5 \\
0 &amp; \text{else}
\end{cases}
\notag
$$<p><em>Hint</em>: what is the derivative of this loss with respect to any prediction $\hat p_i$?</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We just developed a machine learning method out of these three components:</p>
<ul>
<li>linear-logistic model (model)</li>
<li>log loss (loss)</li>
<li>gradient descent (optimization algorithm)</li>
</ul>
<p>This should make it clear to you that gradient descent is a widely-applicable strategy that will work with any loss or model, as long as you can define derivatives of the loss with respect to the model parameters. We will now go back to our linear regression example, but augment the model with more parameters and a more complex structure in order to capture nonlinear relationships.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Adding-complexity-to-our-linear-model">Adding complexity to our linear model<a class="anchor-link" href="#Adding-complexity-to-our-linear-model"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's go back to our regression example.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">5</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (n observations, p features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (n observations)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">β</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> 
<span class="n">β</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> 

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> 
    
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients </span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># take the gradient descent step </span>
        <span class="n">β</span> <span class="o">-=</span> <span class="mf">10e-5</span> <span class="o">*</span> <span class="n">β</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">β</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Remember that the linear model is $\hat y =x\beta_{1:p} + \beta_0$. Let's say we want to add some complexity to this model by figuring out if there are some other predictors we can build (call them $z$) that are a transformation of the predictors $x$ that are better for predicting $y$ in a linear model than $x$ is. To do that, we'll first let $z = g(xW + b)$ with $W$ being a $p \times h$ matrix and $b$ a $1 \times h$ matrix so each row of $z$ ends up with $h$ columns and $g$ is an element-wise ReLU function. The purpose of $W$ and $b$ is to make $h$ predictors out of our original $p$ using a linear transformation. The purpose of $g$ is to add complexity to the model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's dig into this, one step at a time. First we'll investigate the transformation $xW$. As we said, we're trying to build new features from our old features, so let's look at just one of these new features, $z_1$:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$ 
z_1 
=
\left[
\begin{array}{c}
z_{11} \\ z_{12} \\ \vdots \\ z_{1n}
\end{array}
\right]
=
g\left(
\left[
\begin{array}{c}
x_{11} &amp; x_{21} &amp; \cdots &amp; x_{p1} \\
x_{12} &amp; x_{22} &amp; \cdots &amp; x_{p2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{1n} &amp; x_{2n} &amp; \cdots &amp; x_{pn}
\end{array}
\right]
\left[
\begin{array}{c}
w_{11} \\ w_{12} \\ \vdots \\ w_{1p}
\end{array}
\right]
+
b_1
\right)
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Graphically, this looks exactly like logistic regression, but with the $\sigma()$ replaced by $g()$. We also renamed the parameters and output, but that's just cosmetic, not consequential.</p>
<p><img src="images/greg.png" width="250"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we'll be using this structure again and again we'll just abbreviate it as follows:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/greg-abbrev.png" width="130"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that it's implicit here that $w_1$ is a vector of parameters that is being dot-multiplied by the vector $x$, $b_1$ is being added, and then the result is being transformed by some function $g$ (omitted in the drawing).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's zoom back out to our full matrix equation $z = g(xW+b)$. One way to look at this is that we're doing $h$ "$g$-linear" regressions (assuming we have $W$ and $b$) and getting their predictions. The matrix equation $z = g(xW+b)$ expresses all of these "regressions" simultaneously.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="k">import</span> <span class="n">relu</span> <span class="k">as</span> <span class="n">g</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([100, 10])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you see, we're getting 100 observations of 10 "features". Let's look at one of them:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0.0000, 0.0000, 5.0925, 2.2966, 1.3944, 0.0000, 5.4351, 3.0747, 0.7186,
        0.3656], grad_fn=&lt;SelectBackward&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We could also have done them one-at-a-time:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">w1</span> <span class="o">=</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">b</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">z1</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([0.0000, 0.0000, 5.0925, 2.2966, 1.3944, 0.0000, 5.4351, 3.0747, 0.7186,
        0.3656], grad_fn=&lt;SliceBackward&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Same thing.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The graphical representation of this is:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/2layer.png" width="150"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One way to think about $z$ is as a different "representation" of what's in $x$. If we're predicting whether an image is a cat or a dog based on the pixel values $x$, then $z$ will perhaps learn to encode something like the number of red pixels, whether or not there are triangles in the image, etc. Ultimately the two representations contain the same information since $z$ is just a transformation of $x$, but somehow we're looking at that information in a different way, or highlighting different aspects of it.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now our final model is a linear model done on the new predictors $z$. That model is $\hat y =z\alpha_{1:h} + \alpha_0$. I'm calling the coefficient vector $\alpha$ because now this needs $h$ coefficients (plus 1 intercept) for each of the $h$ new features, whereas $\beta$ was a $p$-length vector. So, in full, our model is now $\hat y = g(xW+b)\alpha_{1:h} + \alpha_0$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">α</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">α</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">α</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/net.png" width="300"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that without the ReLU (or some nonlinear function) between $x$ and $z$, there is always a vector $\beta$ such that $x\beta_{1:p} + \beta_0 = (xW+b)\alpha_{1:h} + \alpha_0$ for every $W$, $b$, and $\alpha$. In effect, adding the transformation $W$ doesn't actually add anything to the model- we're expanding the number of parameters to fit without actually expanding the expressivity of the model because there is always a model  with fewer parameters that perfectly captures the predictions of the over-parametrized model. That is why the nonlinearity $g$ is necessary.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE:</strong></p>
<p>Let's say we have</p>
$$
W = 
\left[
\begin{array}{ccc}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 3
\end{array}
\right]    
\quad
b = 
\left[
\begin{array}{ccc}
0 &amp; 0 &amp; 0
\end{array}
\right]
\\
\alpha_{1:3} = 
\left[
\begin{array}{c}
1 \\ -1 \\ 0
\end{array}
\right]
\quad
\alpha_0 = 0
\notag
$$<p>Find $\beta$ so that $x\beta_{1:2} + \beta_0 = (xW+b)\alpha_{1:3} + \alpha_0$ no matter what $x$ is.</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can compute our MSE loss:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
<span class="n">L</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(28.0840, grad_fn=&lt;DivBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input tag_popout">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">make_dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;W&#39;</span><span class="p">:</span><span class="n">W</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span><span class="n">b</span><span class="p">,</span> <span class="s1">&#39;α&#39;</span><span class="p">:</span><span class="n">α</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="321pt" height="568pt"
 viewBox="0.00 0.00 321.31 568.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 564)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-564 317.31,-564 317.31,4 -4,4"/>
<!-- 4928161552 -->
<g id="node1" class="node">
<title>4928161552</title>
<polygon fill="#caff70" stroke="black" points="256.47,-20 168.18,-20 168.18,0 256.47,0 256.47,-20"/>
<text text-anchor="middle" x="212.32" y="-6.4" font-family="Times,serif" font-size="12.00">DivBackward0</text>
</g>
<!-- 4928161936 -->
<g id="node2" class="node">
<title>4928161936</title>
<polygon fill="lightgrey" stroke="black" points="258.48,-76 166.17,-76 166.17,-56 258.48,-56 258.48,-76"/>
<text text-anchor="middle" x="212.32" y="-62.4" font-family="Times,serif" font-size="12.00">SumBackward0</text>
</g>
<!-- 4928161936&#45;&gt;4928161552 -->
<g id="edge1" class="edge">
<title>4928161936&#45;&gt;4928161552</title>
<path fill="none" stroke="black" d="M212.32,-55.59C212.32,-48.7 212.32,-39.1 212.32,-30.57"/>
<polygon fill="black" stroke="black" points="215.82,-30.3 212.32,-20.3 208.82,-30.3 215.82,-30.3"/>
</g>
<!-- 4975658768 -->
<g id="node3" class="node">
<title>4975658768</title>
<polygon fill="lightgrey" stroke="black" points="258.31,-132 166.34,-132 166.34,-112 258.31,-112 258.31,-132"/>
<text text-anchor="middle" x="212.32" y="-118.4" font-family="Times,serif" font-size="12.00">PowBackward0</text>
</g>
<!-- 4975658768&#45;&gt;4928161936 -->
<g id="edge2" class="edge">
<title>4975658768&#45;&gt;4928161936</title>
<path fill="none" stroke="black" d="M212.32,-111.59C212.32,-104.7 212.32,-95.1 212.32,-86.57"/>
<polygon fill="black" stroke="black" points="215.82,-86.3 212.32,-76.3 208.82,-86.3 215.82,-86.3"/>
</g>
<!-- 4975657808 -->
<g id="node4" class="node">
<title>4975657808</title>
<polygon fill="lightgrey" stroke="black" points="257.14,-188 167.51,-188 167.51,-168 257.14,-168 257.14,-188"/>
<text text-anchor="middle" x="212.32" y="-174.4" font-family="Times,serif" font-size="12.00">SubBackward0</text>
</g>
<!-- 4975657808&#45;&gt;4975658768 -->
<g id="edge3" class="edge">
<title>4975657808&#45;&gt;4975658768</title>
<path fill="none" stroke="black" d="M212.32,-167.59C212.32,-160.7 212.32,-151.1 212.32,-142.57"/>
<polygon fill="black" stroke="black" points="215.82,-142.3 212.32,-132.3 208.82,-142.3 215.82,-142.3"/>
</g>
<!-- 4975658192 -->
<g id="node5" class="node">
<title>4975658192</title>
<polygon fill="lightgrey" stroke="black" points="258.14,-244 166.51,-244 166.51,-224 258.14,-224 258.14,-244"/>
<text text-anchor="middle" x="212.32" y="-230.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 4975658192&#45;&gt;4975657808 -->
<g id="edge4" class="edge">
<title>4975658192&#45;&gt;4975657808</title>
<path fill="none" stroke="black" d="M212.32,-223.59C212.32,-216.7 212.32,-207.1 212.32,-198.57"/>
<polygon fill="black" stroke="black" points="215.82,-198.3 212.32,-188.3 208.82,-198.3 215.82,-198.3"/>
</g>
<!-- 4975660880 -->
<g id="node6" class="node">
<title>4975660880</title>
<polygon fill="lightgrey" stroke="black" points="201.47,-300 117.18,-300 117.18,-280 201.47,-280 201.47,-300"/>
<text text-anchor="middle" x="159.32" y="-286.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 4975660880&#45;&gt;4975658192 -->
<g id="edge5" class="edge">
<title>4975660880&#45;&gt;4975658192</title>
<path fill="none" stroke="black" d="M168.56,-279.59C176.16,-271.85 187.11,-260.69 196.16,-251.47"/>
<polygon fill="black" stroke="black" points="198.69,-253.89 203.2,-244.3 193.7,-248.98 198.69,-253.89"/>
</g>
<!-- 4975711760 -->
<g id="node7" class="node">
<title>4975711760</title>
<polygon fill="lightgrey" stroke="black" points="132.13,-356 38.52,-356 38.52,-336 132.13,-336 132.13,-356"/>
<text text-anchor="middle" x="85.32" y="-342.4" font-family="Times,serif" font-size="12.00">ReluBackward0</text>
</g>
<!-- 4975711760&#45;&gt;4975660880 -->
<g id="edge6" class="edge">
<title>4975711760&#45;&gt;4975660880</title>
<path fill="none" stroke="black" d="M97.88,-335.84C109,-327.72 125.46,-315.71 138.54,-306.16"/>
<polygon fill="black" stroke="black" points="140.94,-308.75 146.96,-300.03 136.82,-303.09 140.94,-308.75"/>
</g>
<!-- 4975712208 -->
<g id="node8" class="node">
<title>4975712208</title>
<polygon fill="lightgrey" stroke="black" points="131.14,-418 39.51,-418 39.51,-398 131.14,-398 131.14,-418"/>
<text text-anchor="middle" x="85.32" y="-404.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 4975712208&#45;&gt;4975711760 -->
<g id="edge7" class="edge">
<title>4975712208&#45;&gt;4975711760</title>
<path fill="none" stroke="black" d="M85.32,-397.89C85.32,-389.52 85.32,-376.84 85.32,-366.23"/>
<polygon fill="black" stroke="black" points="88.82,-366.2 85.32,-356.2 81.82,-366.2 88.82,-366.2"/>
</g>
<!-- 4975711376 -->
<g id="node9" class="node">
<title>4975711376</title>
<polygon fill="lightgrey" stroke="black" points="84.47,-486 0.18,-486 0.18,-466 84.47,-466 84.47,-486"/>
<text text-anchor="middle" x="42.32" y="-472.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 4975711376&#45;&gt;4975712208 -->
<g id="edge8" class="edge">
<title>4975711376&#45;&gt;4975712208</title>
<path fill="none" stroke="black" d="M48.3,-465.82C54.85,-455.77 65.51,-439.42 73.71,-426.83"/>
<polygon fill="black" stroke="black" points="76.87,-428.39 79.4,-418.1 71,-424.57 76.87,-428.39"/>
</g>
<!-- 4975714128 -->
<g id="node10" class="node">
<title>4975714128</title>
<polygon fill="lightblue" stroke="black" points="69.32,-560 15.32,-560 15.32,-528 69.32,-528 69.32,-560"/>
<text text-anchor="middle" x="42.32" y="-546.4" font-family="Times,serif" font-size="12.00">W</text>
<text text-anchor="middle" x="42.32" y="-534.4" font-family="Times,serif" font-size="12.00"> (5, 10)</text>
</g>
<!-- 4975714128&#45;&gt;4975711376 -->
<g id="edge9" class="edge">
<title>4975714128&#45;&gt;4975711376</title>
<path fill="none" stroke="black" d="M42.32,-527.69C42.32,-518.4 42.32,-506.44 42.32,-496.47"/>
<polygon fill="black" stroke="black" points="45.82,-496.32 42.32,-486.32 38.82,-496.32 45.82,-496.32"/>
</g>
<!-- 4975711504 -->
<g id="node11" class="node">
<title>4975711504</title>
<polygon fill="lightblue" stroke="black" points="156.32,-492 102.32,-492 102.32,-460 156.32,-460 156.32,-492"/>
<text text-anchor="middle" x="129.32" y="-478.4" font-family="Times,serif" font-size="12.00">b</text>
<text text-anchor="middle" x="129.32" y="-466.4" font-family="Times,serif" font-size="12.00"> (1, 10)</text>
</g>
<!-- 4975711504&#45;&gt;4975712208 -->
<g id="edge10" class="edge">
<title>4975711504&#45;&gt;4975712208</title>
<path fill="none" stroke="black" d="M119.12,-459.69C112.59,-449.9 104.09,-437.15 97.25,-426.89"/>
<polygon fill="black" stroke="black" points="99.99,-424.7 91.54,-418.32 94.17,-428.58 99.99,-424.7"/>
</g>
<!-- 4975710672 -->
<g id="node12" class="node">
<title>4975710672</title>
<polygon fill="lightgrey" stroke="black" points="238.46,-356 150.18,-356 150.18,-336 238.46,-336 238.46,-356"/>
<text text-anchor="middle" x="194.32" y="-342.4" font-family="Times,serif" font-size="12.00">SliceBackward</text>
</g>
<!-- 4975710672&#45;&gt;4975660880 -->
<g id="edge11" class="edge">
<title>4975710672&#45;&gt;4975660880</title>
<path fill="none" stroke="black" d="M188.23,-335.59C183.46,-328.24 176.69,-317.8 170.9,-308.87"/>
<polygon fill="black" stroke="black" points="173.73,-306.79 165.35,-300.3 167.85,-310.59 173.73,-306.79"/>
</g>
<!-- 4975711120 -->
<g id="node13" class="node">
<title>4975711120</title>
<polygon fill="lightblue" stroke="black" points="257.32,-424 203.32,-424 203.32,-392 257.32,-392 257.32,-424"/>
<text text-anchor="middle" x="230.32" y="-410.4" font-family="Times,serif" font-size="12.00">α</text>
<text text-anchor="middle" x="230.32" y="-398.4" font-family="Times,serif" font-size="12.00"> (11, 1)</text>
</g>
<!-- 4975711120&#45;&gt;4975710672 -->
<g id="edge12" class="edge">
<title>4975711120&#45;&gt;4975710672</title>
<path fill="none" stroke="black" d="M221.24,-391.86C216.34,-383.69 210.25,-373.55 205.11,-364.98"/>
<polygon fill="black" stroke="black" points="207.96,-362.93 199.82,-356.15 201.96,-366.53 207.96,-362.93"/>
</g>
<!-- 4975660368 -->
<g id="node14" class="node">
<title>4975660368</title>
<polygon fill="lightgrey" stroke="black" points="313.29,-300 219.36,-300 219.36,-280 313.29,-280 313.29,-300"/>
<text text-anchor="middle" x="266.32" y="-286.4" font-family="Times,serif" font-size="12.00">SelectBackward</text>
</g>
<!-- 4975711120&#45;&gt;4975660368 -->
<g id="edge14" class="edge">
<title>4975711120&#45;&gt;4975660368</title>
<path fill="none" stroke="black" d="M235.53,-391.87C238.97,-381.75 243.54,-368.11 247.32,-356 252.15,-340.57 257.27,-322.96 260.98,-309.97"/>
<polygon fill="black" stroke="black" points="264.37,-310.87 263.73,-300.29 257.63,-308.96 264.37,-310.87"/>
</g>
<!-- 4975660368&#45;&gt;4975658192 -->
<g id="edge13" class="edge">
<title>4975660368&#45;&gt;4975658192</title>
<path fill="none" stroke="black" d="M256.92,-279.59C249.17,-271.85 238.01,-260.69 228.79,-251.47"/>
<polygon fill="black" stroke="black" points="231.17,-248.89 221.62,-244.3 226.22,-253.84 231.17,-248.89"/>
</g>
</g>
</svg>

</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now $L$ depends both on what $W$ and $b$ are and on what $\alpha$ is. In other words, both of these sets of coefficients are parameters of the model that we can tweak and optimize. $W$ and $b$ determine what kinds of "new predictors" we get out $x$, while $\alpha$ puts those new features together into the final prediction. What's amazing is that we can fit all of these parameters simultaneously using gradient descent: just update both sets of parameters using the negative gradient at each step. Note that this model has $h\times (p+1) + h+1$ total parameters (the values of $W_{ij}$, $b_j$, and $\alpha_j$) to be fit, whereas our original linear model only had $p+1$ parameters.</p>
<p>Furthermore, this poses no problem for pytorch:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">α</span><span class="o">.</span><span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ -6.6449],
        [ -6.8241],
        [-12.2024],
        [ -9.9064],
        [ -6.9516],
        [ -3.6036],
        [-20.2889],
        [-12.0027],
        [ -2.1520],
        [-13.9219],
        [-21.4951]])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-3.0923,  1.5987, -3.7242, -3.4357, -0.5628,  3.9108,  4.4897,  0.7864,
          2.4087,  6.8900],
        [-1.9493,  0.4581, -0.2307,  0.3863, -0.5116, -0.9701,  1.3576,  0.4748,
          0.8030,  2.4588],
        [-0.6393,  0.1410, -3.1890, -0.8218, -2.4884,  2.6136,  2.0194,  0.7845,
          2.1102,  6.1789],
        [ 0.4988, -0.7695,  1.0725, -0.2269,  1.2005, -2.5940, -2.8195,  0.7122,
          0.4712, -0.0551],
        [ 2.1704, -0.9893,  0.6358,  2.2685,  1.1096, -3.3306, -4.4946,  0.4656,
         -0.0313,  0.0125]])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-2.3978,  1.5757, -4.6751, -2.9751, -2.2472,  5.1332,  5.5919,  0.7547,
          2.2165,  7.1073]])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Looping">Looping<a class="anchor-link" href="#Looping"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As before, now that we have gone through this once, we can put it all together into a loop:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">5</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (n observations, p features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (n observations)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">α</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">α</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">α</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
    
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients (in this case δL/δα, δL/δW)</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># take the gradient descent step </span>
        <span class="n">α</span> <span class="o">-=</span> <span class="mf">10e-3</span> <span class="o">*</span> <span class="n">α</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">α</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">W</span> <span class="o">-=</span> <span class="mf">10e-3</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="mf">10e-3</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_123_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And here are our predictions for a new $x$:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 10 new observations</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">α</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span><span class="o">+</span><span class="n">α</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.2449],
        [-0.3287],
        [ 0.6254],
        [-0.1357],
        [-0.2929],
        [-0.3676],
        [ 0.2192],
        [ 0.3494],
        [-0.6368],
        [-0.0381]], grad_fn=&lt;AddBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE</strong></p>
<p>Apply this strategy to our logistic regression to make a more complex <strong>classification</strong> model. In other words, implement the machine learning algorithm with these characteristics:</p>
<ul>
<li>Model: $\hat p_i = \sigma(g(x_i W+b))\alpha_{1:h} + \alpha_0)$, with $\sigma$, $g$, $W$, $b$, and $\alpha$ as previously described</li>
<li>Loss: log-loss</li>
<li>Search Algorithm: gradient descent</li>
</ul>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We just developed a machine learning method out of these three components:</p>
<ul>
<li>"linear-relu" $\times$ 10 + linear model (model)</li>
<li>MSE loss (loss)</li>
<li>gradient descent (search algorithm)</li>
</ul>
<p>This is a neural network. The 10 "new predictors" are together a <em>hidden layer</em> (hidden because we don't see them at the end of the day, just the output $\hat y$) and each of them individually is called a <em>hidden unit</em>. The "layer" terminology comes from the graphical representation, where the successive hidden representations look like successive layers that the information passes through while being transformed from predictors to prediction:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/net-layers.png" width="300"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, in the language of neural networks or deep learning, what we've built is a network with input of size 5, one linear hidden layer of size 10, ReLU <em>activation function</em>, and a linear output layer of size 1.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-does-this-help?">How does this help?<a class="anchor-link" href="#How-does-this-help?"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While we could define ever-more complex interactions of parameters and data, it turns out that simply stacking linear regressions with nonlinear activation functions on top of each other (e.g. "linear-relu" regressions) gets us enough complexity to model practically any relationship we want. Let's have a look at an example where $y = x^2$ in reality:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (n observations, p features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (n observations)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_134_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we fit a linear model, we don't do very well:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">β</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># ŷ = xβ + b(calculate predictions)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="c1"># L = Σ(yᵢ-ŷᵢ)² (use predictions to calculate loss)</span>
    
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients (in this case δL/δβ, δL/δW)</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># take the gradient descent step </span>
        <span class="n">β</span> <span class="o">-=</span> <span class="mf">10e-2</span> <span class="o">*</span> <span class="n">β</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">β</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The loss does go down, but overall it's still really high:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_138_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And when we plot the model predictions against reality we see how bad it is:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">ŷ</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_140_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, we can introduce a hidden layer (of size 10):</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">α</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ϵ</span> <span class="o">=</span> <span class="mf">10e-3</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">α</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">α</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="c1"># L = Σ(yᵢ-ŷᵢ)² (use predictions to calculate loss)</span>
    
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients (in this case δL/δα, δL/δW)</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># take the gradient descent step </span>
        <span class="n">α</span> <span class="o">-=</span> <span class="n">ϵ</span> <span class="o">*</span> <span class="n">α</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">α</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">W</span> <span class="o">-=</span> <span class="n">ϵ</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="n">ϵ</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_143_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">ŷ</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_144_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now the loss is lower and the fit is clearly much improved.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE</strong></p>
<p>Plot $z_1$ vs $x$. Repeat for each $z_j$. What do you see?</p>
<p>Now make plots of each $z_j$ vs. $y$. Is the relationship between each $z_j$ and $y$ closer to linear than the relationship between $x$ and $y$? Does this explain why the linear regression $\hat y = \alpha_{1:h} z + \alpha_0$ produces a better fit than the linear regression $\hat y = \beta_{1:p} x + \beta_0$?</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>EXERCISE</strong></p>
<p>Change the value of $h$ in the code above so that hidden layer has size 3 instead of 10. How does that change the fit? What if you set $h=100$?</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>EXERCISE</strong></p>
<p>When fitting linear models, it's a common practice to break continuous variables up into categories. For instance, let's say we're predicting someone's weight based on their age. Instead of modeling $\text{weight} = \beta_0 + \beta_1 \times \text{age}$, we could break age up into 10 categories (say: age &lt; 10 years, 10 &lt; age &lt; 20,... 90 &lt; age) and fit our model using those categories as predictors instead. Basically, we have a set of regions $A = \{(0,10],(10,20],\dots (90,\infty]\}$ and our new variables are $z_j = I_{A_j}(x)$ where $I_{A_j}$ is the indicator function that returns 1 if $x \in A_j$ and 0 otherwise. Now we're modeling $\text{weight} = \alpha_0 + \alpha_1 z_1 + \dots \alpha_{10} z_{10}$ instead of $\text{weight} = \beta_0 + \beta_1 \times \text{age}$.</p>
<p>Considering a population including both adults and children, do you think the relationship between age and weight will be linear? What will that mean if we try to fit the model $\text{weight} = \beta_0 + \beta_1 \times \text{age}$?</p>
<p>How is this problem addressed by transforming $x$ into 10 new predictors $z_j$ using the indicator functions? What is the relationship between any $z_j$ and $y$?</p>
<p>How does this relate to our neural networks?</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We saw how we can get a nonlinear fit out of stacking linear models and nonlinear activation functions together.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Abstracting-the-&quot;Layer&quot;">Abstracting the "Layer"<a class="anchor-link" href="#Abstracting-the-&quot;Layer&quot;"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we'll be stacking layers of arbitrary size arbitrarily deep, we might as well abstract away some of the code we need so we don't have to keep track of all the parameters in the model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll first define a <code>LinearLayer</code> class that transforms a representation of size <code>h1</code> to one of size <code>h2</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span><span class="n">h2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">h2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ϵ</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">ϵ</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">-=</span> <span class="n">ϵ</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>        
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now we can stack up as many of these as we want, of whatever sizes we want</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="k">import</span> <span class="n">relu</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span><span class="n">h2</span><span class="p">)</span> <span class="k">for</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">relu</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ϵ</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">ϵ</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>check it out:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span><span class="o">=</span> <span class="mi">100</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (n observations, p features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (n observations)</span>

<span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_popout tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Ws</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="s1">&#39;W</span><span class="si">{k}</span><span class="s1">&#39;</span><span class="p">:</span><span class="n">layer</span><span class="o">.</span><span class="n">W</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">layers</span><span class="p">)}</span>
<span class="n">bs</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="s1">&#39;b</span><span class="si">{k}</span><span class="s1">&#39;</span><span class="p">:</span><span class="n">layer</span><span class="o">.</span><span class="n">b</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">layers</span><span class="p">)}</span>
<span class="n">make_dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="p">{</span><span class="o">**</span><span class="n">Ws</span><span class="p">,</span> <span class="o">**</span><span class="n">bs</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="342pt" height="772pt"
 viewBox="0.00 0.00 342.32 772.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 768)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-768 338.32,-768 338.32,4 -4,4"/>
<!-- 5032136464 -->
<g id="node1" class="node">
<title>5032136464</title>
<polygon fill="#caff70" stroke="black" points="307.47,-20 219.18,-20 219.18,0 307.47,0 307.47,-20"/>
<text text-anchor="middle" x="263.32" y="-6.4" font-family="Times,serif" font-size="12.00">DivBackward0</text>
</g>
<!-- 5032218704 -->
<g id="node2" class="node">
<title>5032218704</title>
<polygon fill="lightgrey" stroke="black" points="309.48,-76 217.17,-76 217.17,-56 309.48,-56 309.48,-76"/>
<text text-anchor="middle" x="263.32" y="-62.4" font-family="Times,serif" font-size="12.00">SumBackward0</text>
</g>
<!-- 5032218704&#45;&gt;5032136464 -->
<g id="edge1" class="edge">
<title>5032218704&#45;&gt;5032136464</title>
<path fill="none" stroke="black" d="M263.32,-55.59C263.32,-48.7 263.32,-39.1 263.32,-30.57"/>
<polygon fill="black" stroke="black" points="266.82,-30.3 263.32,-20.3 259.82,-30.3 266.82,-30.3"/>
</g>
<!-- 5032218832 -->
<g id="node3" class="node">
<title>5032218832</title>
<polygon fill="lightgrey" stroke="black" points="309.31,-132 217.34,-132 217.34,-112 309.31,-112 309.31,-132"/>
<text text-anchor="middle" x="263.32" y="-118.4" font-family="Times,serif" font-size="12.00">PowBackward0</text>
</g>
<!-- 5032218832&#45;&gt;5032218704 -->
<g id="edge2" class="edge">
<title>5032218832&#45;&gt;5032218704</title>
<path fill="none" stroke="black" d="M263.32,-111.59C263.32,-104.7 263.32,-95.1 263.32,-86.57"/>
<polygon fill="black" stroke="black" points="266.82,-86.3 263.32,-76.3 259.82,-86.3 266.82,-86.3"/>
</g>
<!-- 5032218960 -->
<g id="node4" class="node">
<title>5032218960</title>
<polygon fill="lightgrey" stroke="black" points="308.14,-188 218.51,-188 218.51,-168 308.14,-168 308.14,-188"/>
<text text-anchor="middle" x="263.32" y="-174.4" font-family="Times,serif" font-size="12.00">SubBackward0</text>
</g>
<!-- 5032218960&#45;&gt;5032218832 -->
<g id="edge3" class="edge">
<title>5032218960&#45;&gt;5032218832</title>
<path fill="none" stroke="black" d="M263.32,-167.59C263.32,-160.7 263.32,-151.1 263.32,-142.57"/>
<polygon fill="black" stroke="black" points="266.82,-142.3 263.32,-132.3 259.82,-142.3 266.82,-142.3"/>
</g>
<!-- 5032219152 -->
<g id="node5" class="node">
<title>5032219152</title>
<polygon fill="lightgrey" stroke="black" points="309.14,-244 217.51,-244 217.51,-224 309.14,-224 309.14,-244"/>
<text text-anchor="middle" x="263.32" y="-230.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 5032219152&#45;&gt;5032218960 -->
<g id="edge4" class="edge">
<title>5032219152&#45;&gt;5032218960</title>
<path fill="none" stroke="black" d="M263.32,-223.59C263.32,-216.7 263.32,-207.1 263.32,-198.57"/>
<polygon fill="black" stroke="black" points="266.82,-198.3 263.32,-188.3 259.82,-198.3 266.82,-198.3"/>
</g>
<!-- 5032219280 -->
<g id="node6" class="node">
<title>5032219280</title>
<polygon fill="lightgrey" stroke="black" points="262.47,-306 178.18,-306 178.18,-286 262.47,-286 262.47,-306"/>
<text text-anchor="middle" x="220.32" y="-292.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 5032219280&#45;&gt;5032219152 -->
<g id="edge5" class="edge">
<title>5032219280&#45;&gt;5032219152</title>
<path fill="none" stroke="black" d="M226.85,-285.89C233.17,-277.08 242.9,-263.5 250.73,-252.58"/>
<polygon fill="black" stroke="black" points="253.75,-254.37 256.73,-244.2 248.06,-250.29 253.75,-254.37"/>
</g>
<!-- 5032219408 -->
<g id="node7" class="node">
<title>5032219408</title>
<polygon fill="lightgrey" stroke="black" points="221.13,-374 127.52,-374 127.52,-354 221.13,-354 221.13,-374"/>
<text text-anchor="middle" x="174.32" y="-360.4" font-family="Times,serif" font-size="12.00">ReluBackward0</text>
</g>
<!-- 5032219408&#45;&gt;5032219280 -->
<g id="edge6" class="edge">
<title>5032219408&#45;&gt;5032219280</title>
<path fill="none" stroke="black" d="M180.72,-353.82C187.79,-343.68 199.35,-327.1 208.16,-314.46"/>
<polygon fill="black" stroke="black" points="211.14,-316.3 213.98,-306.1 205.39,-312.3 211.14,-316.3"/>
</g>
<!-- 5032219600 -->
<g id="node8" class="node">
<title>5032219600</title>
<polygon fill="lightgrey" stroke="black" points="220.14,-436 128.51,-436 128.51,-416 220.14,-416 220.14,-436"/>
<text text-anchor="middle" x="174.32" y="-422.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 5032219600&#45;&gt;5032219408 -->
<g id="edge7" class="edge">
<title>5032219600&#45;&gt;5032219408</title>
<path fill="none" stroke="black" d="M174.32,-415.89C174.32,-407.52 174.32,-394.84 174.32,-384.23"/>
<polygon fill="black" stroke="black" points="177.82,-384.2 174.32,-374.2 170.82,-384.2 177.82,-384.2"/>
</g>
<!-- 5032219728 -->
<g id="node9" class="node">
<title>5032219728</title>
<polygon fill="lightgrey" stroke="black" points="173.47,-498 89.18,-498 89.18,-478 173.47,-478 173.47,-498"/>
<text text-anchor="middle" x="131.32" y="-484.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 5032219728&#45;&gt;5032219600 -->
<g id="edge8" class="edge">
<title>5032219728&#45;&gt;5032219600</title>
<path fill="none" stroke="black" d="M137.85,-477.89C144.17,-469.08 153.9,-455.5 161.73,-444.58"/>
<polygon fill="black" stroke="black" points="164.75,-446.37 167.73,-436.2 159.06,-442.29 164.75,-446.37"/>
</g>
<!-- 5032219920 -->
<g id="node10" class="node">
<title>5032219920</title>
<polygon fill="lightgrey" stroke="black" points="132.13,-566 38.52,-566 38.52,-546 132.13,-546 132.13,-566"/>
<text text-anchor="middle" x="85.32" y="-552.4" font-family="Times,serif" font-size="12.00">ReluBackward0</text>
</g>
<!-- 5032219920&#45;&gt;5032219728 -->
<g id="edge9" class="edge">
<title>5032219920&#45;&gt;5032219728</title>
<path fill="none" stroke="black" d="M91.72,-545.82C98.79,-535.68 110.35,-519.1 119.16,-506.46"/>
<polygon fill="black" stroke="black" points="122.14,-508.3 124.98,-498.1 116.39,-504.3 122.14,-508.3"/>
</g>
<!-- 5032220112 -->
<g id="node11" class="node">
<title>5032220112</title>
<polygon fill="lightgrey" stroke="black" points="131.14,-628 39.51,-628 39.51,-608 131.14,-608 131.14,-628"/>
<text text-anchor="middle" x="85.32" y="-614.4" font-family="Times,serif" font-size="12.00">AddBackward0</text>
</g>
<!-- 5032220112&#45;&gt;5032219920 -->
<g id="edge10" class="edge">
<title>5032220112&#45;&gt;5032219920</title>
<path fill="none" stroke="black" d="M85.32,-607.89C85.32,-599.52 85.32,-586.84 85.32,-576.23"/>
<polygon fill="black" stroke="black" points="88.82,-576.2 85.32,-566.2 81.82,-576.2 88.82,-576.2"/>
</g>
<!-- 5032220240 -->
<g id="node12" class="node">
<title>5032220240</title>
<polygon fill="lightgrey" stroke="black" points="84.47,-690 0.18,-690 0.18,-670 84.47,-670 84.47,-690"/>
<text text-anchor="middle" x="42.32" y="-676.4" font-family="Times,serif" font-size="12.00">MmBackward</text>
</g>
<!-- 5032220240&#45;&gt;5032220112 -->
<g id="edge11" class="edge">
<title>5032220240&#45;&gt;5032220112</title>
<path fill="none" stroke="black" d="M48.85,-669.89C55.17,-661.08 64.9,-647.5 72.73,-636.58"/>
<polygon fill="black" stroke="black" points="75.75,-638.37 78.73,-628.2 70.06,-634.29 75.75,-638.37"/>
</g>
<!-- 5032220432 -->
<g id="node13" class="node">
<title>5032220432</title>
<polygon fill="lightblue" stroke="black" points="69.32,-764 15.32,-764 15.32,-732 69.32,-732 69.32,-764"/>
<text text-anchor="middle" x="42.32" y="-750.4" font-family="Times,serif" font-size="12.00">W0</text>
<text text-anchor="middle" x="42.32" y="-738.4" font-family="Times,serif" font-size="12.00"> (1, 10)</text>
</g>
<!-- 5032220432&#45;&gt;5032220240 -->
<g id="edge12" class="edge">
<title>5032220432&#45;&gt;5032220240</title>
<path fill="none" stroke="black" d="M42.32,-731.69C42.32,-722.4 42.32,-710.44 42.32,-700.47"/>
<polygon fill="black" stroke="black" points="45.82,-700.32 42.32,-690.32 38.82,-700.32 45.82,-700.32"/>
</g>
<!-- 5032220304 -->
<g id="node14" class="node">
<title>5032220304</title>
<polygon fill="lightblue" stroke="black" points="156.32,-696 102.32,-696 102.32,-664 156.32,-664 156.32,-696"/>
<text text-anchor="middle" x="129.32" y="-682.4" font-family="Times,serif" font-size="12.00">b0</text>
<text text-anchor="middle" x="129.32" y="-670.4" font-family="Times,serif" font-size="12.00"> (1, 10)</text>
</g>
<!-- 5032220304&#45;&gt;5032220112 -->
<g id="edge13" class="edge">
<title>5032220304&#45;&gt;5032220112</title>
<path fill="none" stroke="black" d="M118.22,-663.86C112.1,-655.51 104.47,-645.11 98.1,-636.42"/>
<polygon fill="black" stroke="black" points="100.77,-634.15 92.04,-628.15 95.13,-638.29 100.77,-634.15"/>
</g>
<!-- 5032219984 -->
<g id="node15" class="node">
<title>5032219984</title>
<polygon fill="lightblue" stroke="black" points="206.82,-572 149.83,-572 149.83,-540 206.82,-540 206.82,-572"/>
<text text-anchor="middle" x="178.32" y="-558.4" font-family="Times,serif" font-size="12.00">W1</text>
<text text-anchor="middle" x="178.32" y="-546.4" font-family="Times,serif" font-size="12.00"> (10, 10)</text>
</g>
<!-- 5032219984&#45;&gt;5032219728 -->
<g id="edge14" class="edge">
<title>5032219984&#45;&gt;5032219728</title>
<path fill="none" stroke="black" d="M167.42,-539.69C160.38,-529.8 151.18,-516.89 143.84,-506.57"/>
<polygon fill="black" stroke="black" points="146.61,-504.43 137.96,-498.32 140.91,-508.49 146.61,-504.43"/>
</g>
<!-- 5032219792 -->
<g id="node16" class="node">
<title>5032219792</title>
<polygon fill="lightblue" stroke="black" points="245.32,-504 191.32,-504 191.32,-472 245.32,-472 245.32,-504"/>
<text text-anchor="middle" x="218.32" y="-490.4" font-family="Times,serif" font-size="12.00">b1</text>
<text text-anchor="middle" x="218.32" y="-478.4" font-family="Times,serif" font-size="12.00"> (1, 10)</text>
</g>
<!-- 5032219792&#45;&gt;5032219600 -->
<g id="edge15" class="edge">
<title>5032219792&#45;&gt;5032219600</title>
<path fill="none" stroke="black" d="M207.22,-471.86C201.1,-463.51 193.47,-453.11 187.1,-444.42"/>
<polygon fill="black" stroke="black" points="189.77,-442.15 181.04,-436.15 184.13,-446.29 189.77,-442.15"/>
</g>
<!-- 5032219472 -->
<g id="node17" class="node">
<title>5032219472</title>
<polygon fill="lightblue" stroke="black" points="293.32,-380 239.32,-380 239.32,-348 293.32,-348 293.32,-380"/>
<text text-anchor="middle" x="266.32" y="-366.4" font-family="Times,serif" font-size="12.00">W2</text>
<text text-anchor="middle" x="266.32" y="-354.4" font-family="Times,serif" font-size="12.00"> (10, 1)</text>
</g>
<!-- 5032219472&#45;&gt;5032219280 -->
<g id="edge16" class="edge">
<title>5032219472&#45;&gt;5032219280</title>
<path fill="none" stroke="black" d="M255.66,-347.69C248.76,-337.8 239.76,-324.89 232.57,-314.57"/>
<polygon fill="black" stroke="black" points="235.41,-312.52 226.82,-306.32 229.66,-316.52 235.41,-312.52"/>
</g>
<!-- 5032219344 -->
<g id="node18" class="node">
<title>5032219344</title>
<polygon fill="lightblue" stroke="black" points="334.32,-312 280.32,-312 280.32,-280 334.32,-280 334.32,-312"/>
<text text-anchor="middle" x="307.32" y="-298.4" font-family="Times,serif" font-size="12.00">b2</text>
<text text-anchor="middle" x="307.32" y="-286.4" font-family="Times,serif" font-size="12.00"> (1, 1)</text>
</g>
<!-- 5032219344&#45;&gt;5032219152 -->
<g id="edge17" class="edge">
<title>5032219344&#45;&gt;5032219152</title>
<path fill="none" stroke="black" d="M296.22,-279.86C290.1,-271.51 282.47,-261.11 276.1,-252.42"/>
<polygon fill="black" stroke="black" points="278.77,-250.15 270.04,-244.15 273.13,-254.29 278.77,-250.15"/>
</g>
</g>
</svg>

</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">ϵ</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="mf">10e-4</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="n">my_model</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">ϵ</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_161_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_162_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's easy to get predictions on new data:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">my_model</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.1561],
        [ 0.3794],
        [ 0.2540],
        [-0.0810],
        [ 1.2983],
        [ 1.0457],
        [ 1.4800],
        [ 0.2456],
        [ 0.0334],
        [ 2.5794]], grad_fn=&lt;AddBackward0&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because our code is modular, it's trivial to add more layers or change their sizes. Let's also a new dataset with a different relationship between $x$ and $y$ to see we can fit pretty arbitrary nonlinearities.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># predictors (n observations, p features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># outcomes (n observations)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">ϵ</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="mf">10e-4</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="n">my_model</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">ϵ</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;i&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">),</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span><span class="n">loss_record</span><span class="p">})</span>
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_168_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_169_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Using-pytorch's-modules">Using pytorch's modules<a class="anchor-link" href="#Using-pytorch's-modules"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of implementing our own linear layer class, we can use the one that pytorch provides. We can also subclass <code>nn.Module</code> to make a model without having to define our own <code>step()</code> methods. All we need to do is define a <code>forward()</code> method that, given the input to the model, produces the output:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span> <span class="c1"># subclass nn.Module</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span> <span class="c1"># we need to initialize the nn.Module superclass for things to work</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span><span class="n">h2</span><span class="p">)</span> <span class="k">for</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># creates a model object with input size 1, two hidden layers of size 10, and an output of size 1</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># the loss function in torch is often named &quot;criterion&quot;</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="o">*</span><span class="mf">10e-4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_175_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">my_model</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span> <span class="c1"># predictions</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.0177],
        [ 0.6344],
        [ 0.3463],
        [-1.7594],
        [ 0.5504],
        [ 0.0237],
        [ 0.0724],
        [ 0.3437],
        [-6.9198],
        [-1.2020]], grad_fn=&lt;AddmmBackward&gt;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pytorch has already implementing things like the ReLU and sigmoid functions, MSE, cross-entropy loss, linear layers, etc. So you don't have to worry about making errors implementing them yourself. Just figure out how to use their implementations and you're good to go. Don't worry too much about why you need to do specific things like <code>super().__init__()</code> when initializing a subclass of <code>nn.Module()</code>. As long as you understand <em>what</em> is happening under the hood, it doesn't really matter if you don't know exactly <em>how</em>. If you run into a problem where you need to figure it out, you will easily be able to if you need to do so.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We've been using pytorch so far to automate gradients, but now we've also seen that the <code>optim</code> module can be used to even automate the process of taking the gradient descent step. Basically, when you create an optimizer object like <code>optimizer = optim.SGD(my_model.parameters(), lr=1*10e-4)</code>, it associates with itself all of the model parameters that are accessible through <code>my_model.parameters()</code>. <code>parameters()</code> is a method that is available to <code>my_model</code> because you subclassed <code>nn.Module</code>, which ran through the internals of <code>my_model</code> to find all of the parameters associated with all of the layers. Now when you call <code>optimizer.step()</code> after populating all of the <code>param.grad</code> (which happens when you do <code>L.backward()</code>), the optimizer applies some algorithm to take all the gradients and change the values of the parameters accordingly. The simplest algorithm is the standard gradient descent we've been using, but many other enhancements are also available (see the <a href="https://github.com/alejandroschuler/articles/blob/master/grad_descent/Gradient%20Descent.ipynb">gradient descent primer</a> to understand the details). For instance, adding momentum is easy:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> 
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># the loss function in torch is often named &quot;criterion&quot;</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="o">*</span><span class="mf">10e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_181_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice we didn't have to change the training loop at all- we just passed an extra parameter to the optimizer. We can also make more radical changes to the optimizer (in this case we'll use the <a href="https://ruder.io/optimizing-gradient-descent/index.html#adam">ADAM algorithm</a>, another form of gradient descent):</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> 
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># the loss function in torch is often named &quot;criterion&quot;</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="o">*</span><span class="mf">10e-5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_185_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Minibatching-and-Dataloaders">Minibatching and Dataloaders<a class="anchor-link" href="#Minibatching-and-Dataloaders"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So far we've been using all of the data to calculate the gradients in each gradient descent step. However, doing that requires us to have all of the data in-memory at each step to look at $y_i$ and calculate $\hat y_i$ for every single $i$. Mini-batch stochastic gradient descent avoids this problem: in each iteration, we only use a subset of the data (a <em>batch</em> or <em>minibatch</em>) to calculate the loss, and consequently the gradients. It turns out that doing this also helps the optimization algorithm get out of local minima, so it's a win-win (more details in the <a href="https://github.com/alejandroschuler/articles/blob/master/grad_descent/Gradient%20Descent.ipynb">gradient descent primer</a> and <a href="https://ruder.io/optimizing-gradient-descent/index.html#minibatchgradientdescent">overview of gradient descent methods</a>).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In practice, people <em>always</em> use batching when training neural networks, so pytorch makes it easy to do so:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">DataLoader</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> 
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># the loss function in torch is often named &quot;criterion&quot;</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="o">*</span><span class="mf">10e-5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>DataLoader</code> splits the provided data into batches. We iterate through each batch, updating the paramters with a gradient descent step each time. Once we've looked at each batch once (i.e. we've gone through the full datasets once), we call that an <em>epoch</em> and start again.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">loss_record</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">trainloader</span><span class="p">:</span>
        <span class="n">ŷ_batch</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span> 
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_batch</span><span class="p">,</span> <span class="n">ŷ_batch</span><span class="p">)</span>
        <span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">full_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">loss_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">full_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">my_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x1&#39;</span><span class="p">:</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]})</span>
<span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span> <span class="o">+</span> 
<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/linear_reg_torch_193_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Final-Exercises">Final Exercises<a class="anchor-link" href="#Final-Exercises"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Regression">Regression<a class="anchor-link" href="#Regression"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's use everything we know to build some models. The first dataset we'll use is data on different wines.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To start with, we'll download and read in the data into a pandas dataframe.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;;&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Feel free to do any exploratory data analysis or investigate what's in the dataset using any tools you like. You'll see there are 11 predictors, all of which are numeric. There is one target (the "quality" of the wine), which we'll also treat as numeric.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before building a model, I've turned the data into tensors and put it into a pytorch <code>DataLoader</code> for you</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">DataLoader</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s1">&#39;quality&#39;</span><span class="p">]]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span>

<span class="c1"># split into training and test sets, converting data frames to torch tensors along the way</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data_frame</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">for</span> <span class="n">data_frame</span> <span class="ow">in</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">)]</span>

<span class="c1"># make a dataloader so we can iterate over batches</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Feel free to play around with the code above to figure out what it's doing. I didn't normalize the features or outcome, but if you wanted to, this would be the place to do it.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE</strong></p>
<p>Use pytorch to write a 3-layer neural network class (call it <code>WineNet</code>), with ReLU activations in between layers 1 and 2 and between 2 and 3. The output of the 3rd layer will be your prediction. Expect an input to the first layer of size 11 (the number of predictors we have). The output sizes of the 1st and 2nd layers should be variables that the user can set when they instantiate the class, like <code>model = WineNet(100, 50)</code>. The 3rd layer should have an ouput of size 1, since it's $\hat y$.</p>
<p>To test your model, initialize it with some sizes of hidden layers (say, 100, 50), and run the training predictors through it and see what output you get. The predictions will be garbage since the model is untrained, but you can see if it all goes smoothly and your model is ready for training.</p>
<p>Use the pytorch <code>Adam</code> optimizer with 1e-5 learning rate to fit your model to the wine data (using the provided <code>trainloader</code>) by minimizing the mean-squared-error loss with your model, using 100 and 50 as the hidden layer sizes. Write a training loop to fit your model for 100 epochs. Record the value of the training loss at the end of each epoch.</p>
<p>Make a plot of the epoch (x-axis) vs. training loss (y-axis). Calculate the model's predictions on the test set and use the predictions to calculate the mean-squared error on the test set.</p>
<hr>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Classification">Classification<a class="anchor-link" href="#Classification"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's build a model for a binary outcome. The predictors in this dataset are characteristics of various patients along with information extracted from potential tumors. For each patient, the outcome is whether the tumor was benign or malignant.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_breast_cancer</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>EXERCISE</strong></p>
<p>Split these data into a training (70%) and test set (30%), convert all the data to torch tensors, and make a training dataloader.</p>
<p>Use pytorch to write a neural network class (call it <code>BCNet</code>) with any architechture (configuration of layers and activations) you like. As we did in our logistic regression example, we need to make sure there is a sigmoid function after the last linear layer to ensure that the predictions that come out of the model are probabilities. Use any loss function or optimizer that you think are reasonable. Fit your model.</p>
<p>Calculate the model's predictions (probabilities) on the test set. Use 0.5 as a threshold to binarize the probabilities into 'predicted malignant' and 'predicted benign' categories. For what proportion of the patients in the test set is your prediction correct (this is the <em>accuracy</em> metric)? Among those who were really malignant, how many did your model actually predict were malignant (this is the <em>recall</em> or <em>sensitivity</em>)? Among those who your model predicted were malignant, how many actually were malignant (this is the <em>precision</em>)?</p>
<p>Make changes to your architechture, training loss, and/or optimizer. How do your results change? See if there are changes you can make that consistently improve your performance in one or all of these metrics.</p>
<hr>

</div>
</div>
</div>
</div>

 


    </main>
    
            </div>
            <div class="c-textbook__footer" id="textbook_footer">
              
<nav class="c-page__nav">
  
    
    

    <a id="js-page__nav__prev" class="c-page__nav__prev" href="">
      〈 <span class="u-margin-right-tiny"></span> 
    </a>
  

  
    

    
    <a id="js-page__nav__next" class="c-page__nav__next" href="">
       <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

              <footer>
  <p class="footer"></p>
</footer>

            </div>

        </div>
      </main>
    </div>
  </body>
</html>
