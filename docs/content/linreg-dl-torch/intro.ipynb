{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Linear Regression to Deep Learning in Pytorch\n",
    "\n",
    "In this chapter we'll see how to build supervised learning models of seemingly arbitrary complexity without having to manually specify a particular parametric form. The key to this is to stack regression models on top of each other so that the outputs of previous models become the inputs to the subsequent models. But instead of training these models up one-at-a-time, we can use gradient descent to simultaneously search the combined parameter spaces of each of these \"layers\". Deriving the expressions for the gradients of the loss relative to each of these parameters would be incredibly tedious, so we will introduce an auto-differentiation tool provided by the `torch` pacakge that will make gradient descent a breeze to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
