---
interact_link: content/model-eval/model-eval.ipynb
kernel_name: python3
kernel_path: content/model-eval
has_widgets: false
title: |-
  Model Evaluation
pagenum: 10
prev_page:
  url: /linreg-dl-torch/abstracting-layers.html
next_page:
  url: 
suffix: .ipynb
search: f mathcal error model data training test s generalization e y dagger x set our hat fit t using not evaluation well possible metric n because future function its estimate since also point size mathbb tr random space same lets better errors src png width alert best get between gen only much distance models m loss likely performance average good where why smaller around any problem observed even div dont evaluate problems case instead sum happens linear points tdimg td k actually think want used cross frac cant thats approximation sets distribution generating into end isnt regression less might however

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Model Evaluation</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="alert alert-block alert-info">
    As of IPython 2.0, the user interface has changed significantly
</div>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="alert alert-block alert-success">
Enter edit mode by pressing `Enter`
</div><div class="alert alert-success">
Enter edit mode by pressing `Enter`
</div><div class="alert alert-block alert-danger">
Don't try to type into a cell in command mode
</div><div class="alert alert-block alert-warning">
Don't try to type into a cell in command mode
</div>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At the end of the day, we're looking for a model that will help us predict accurately in the future. So the best way to evaluate a model would be to travel forward in time from the time of model deployment until the end of time, gather up all of the model predictions, and compare them to what actually ended up happening. Obviously this isn't feasible, but we need to clearly identify what it is we're trying to optimize before we can figure out a way to get around the practical problems in estimating it.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluation-Metrics">Evaluation Metrics<a class="anchor-link" href="#Evaluation-Metrics"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first thing we need to do is iron out the details of what we mean by "comparing" the predictions to what actually happened. To do that, we need to come up with a rule that tells us how happy we are with a prediction $\hat y$ (in the case of regression) or $\hat p$ (in the case of probabilistic classification) if the real outcome was $y$. We'll call that rule an <em>evaluation metric</em> and denote it $M(y,\hat y)$. The absolute error $|\hat y - y|$ is one possible metric in the case of regression, as is accuracy $(\hat p &gt; 0.5) == y$ in the case of classification. The purpose of the evaluation metric is to encode what <em>you</em> think makes a good prediction, so the sky is the limit. Maybe if you predict within 0.1 of the truth it's all the same to you, but if you're further out than that then you want to penalize quadratically. Maybe after a certain amount of error all predictions are equally bad so you don't care to penalize any more than some maximum value. It's up to you to decide.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="EXERCISE">EXERCISE<a class="anchor-link" href="#EXERCISE"> </a></h5><p>Write python code for an evaluation metric <code>def M(y, ŷ):</code> that returns $|\hat y - y|$ when that value is less than 10, but returns 10 otherwise. Think up a scenario where you might want to use an evaluation metric like this.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="EXERCISE">EXERCISE<a class="anchor-link" href="#EXERCISE"> </a></h5><p>Imagine you're using some kind of regression model to predict the roll angle (in degrees) of a drone based on video from the drone (i.e. is the drone upside down? tilting 30 degrees left?). The regression model can theoretically output any number between minus and positive infinity. One of your colleagues proposes using absolute deviation $|\hat y -y|$ as the evaluation metric, but another colleague instead proposes <code>def M(y, ŷ): np.abs(y-ŷ) % 360</code>. Which do you think is better and why?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Metrics-and-Losses">Metrics and Losses<a class="anchor-link" href="#Metrics-and-Losses"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The evaluation metric is a lot like a loss function in that it tells you how bad or good your prediction is relative to reality. In many cases, the evaluation metric and the loss function used to fit the model in the first place are the same (e.g. mean-squared error for both). However, there are many cases in which they are not the same. There are several reasons why you might want to use a different loss function than your evaluation metric. One common reason is that the evaluation metric you want to use is not differentiable, but the optimization algorithm you are using requires gradients. This is often the case with classification: perhaps the 1/0 classification accuracy is the metric you really care about, but, since it is not differentiable, you use the log loss (cross entropy) instead. Another possible reason is that your evaluation metric includes interactions between several model predictions and outcomes instead of just comparing them one-by-one (we won't discuss metrics like these, but know you can construct them). Or perhaps you would like to use a particular software package that has a particular loss function built into the algorithm, so you aren't free to choose it.</p>
<p>All in all, you can think of the loss as a "dumbed down" version of the evaluation metric. The evaluation metric is the thing you really care about, but for the purpose of fitting a model, you're willing to use a (simpler) loss as a proxy.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generalization-Error">Generalization Error<a class="anchor-link" href="#Generalization-Error"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have a way to quantify how good or how bad a prediction is relative to the truth, we can get back to our model evaluation problem. Let's imagine that between now and the end of time, we would gather the data $(X^{\dagger}, y^{\dagger})$, where the $\dagger$ is just there to differentiate these data from what we've observed in the past. These datasets could be either finite or infinite based on how many times we expect the model to run in the future, but either way let's call the total number of future model runs $n^\dagger$. Let's also say that the model that we've built/trained today is baked into a function $f(x)$ which in code looks something like <code>def f(x): model.predict(x)</code>, and that we've chosen an evaluation metric $M$. Then our final measure of how good our model is should be:</p>
$$
\mathcal E(f) 
=
\frac{1}{n^\dagger} \sum^{n^\dagger}_i M(y_i^\dagger, f(x_i^\dagger))
\label{gen-error}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This quantity is called the <em>generalization error</em> of the model $f$.</p>
<p>The obvious problem with this is that we need to evaluate the model <em>now</em>. We don't have the data from the future at this moment, and we can't afford to wait until the end of time (also, if $n^\dagger$ is infinite, the sum might not even make sense). So because we can't calculate it directly, we have to find a way to <em>approximate</em> the generalization error.</p>
<p>Approximating the generalization error is <em>the</em> central problem of supervised learning. Anyone can come up with a model that produces predictions of one kind or another (using either machine learning or hand-coding), but without quantifying how good this model is likely to perform in the future, it's totally useless.</p>
<p>Moreover, there is no "wrong" way to fit a model. The consequence of doing a bad job fitting a model is that your model may perform poorly. But that's ok! If you evaluate correctly, you know exactly how poor the performance is and you can try different approaches to try to improve it. On the other hand, there are many wrong ways to approximate the generalization error of a model. If you assess your model incorrectly, you will have absolutely no idea of how good your model is in reality. Furthermore, unless you think critically about the result you're seeing, you may not even have any indication that you did something wrong.</p>
<p>This point is so important that it bears repeating: knowing how to <em>evaluate</em> a model is more important than knowing how to <em>fit</em> a model. You should only worry about the latter once you understand the former. So keep your eye on the prize!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Estimating-Generalization-Error">Estimating Generalization Error<a class="anchor-link" href="#Estimating-Generalization-Error"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We want to calculate the generalization error for our model but we can't because we don't have all the future data. One approach we can use to estimate the generalization error is to calculate it using <em>past</em> data instead of <em>future</em> data. In essence, what we are assuming is that what has happened is a pretty good approximation for what will happen. The extent to which this is not true is the extent to which our approximation will be off. We'll use $\hat{\mathcal E}(f)$ to denote this approximation of the generalization error:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\mathcal {\hat E}(f) 
=
\frac{1}{n} \sum^{n}_i M(y_i, f(x_i))
\label{empirical-error}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's try this approach out with a simulation. We're going to build a little "data factory" that generates rows of data for us. We'll run it $n=1000$ times to generate our observed data $(X,y)$. Then we're going to fit a model using that data, which will give us $f$. Once we have our model, we'll estimate the generalization error $\hat{\mathcal E}(f)$ using the above formula \ref{empirical-error}.</p>
<p>However, since our data is simulated, we can also simulate all of the future data $(X^\dagger,y^\dagger)$ that we will observe if we let the model run $n^\dagger=100000$ more times. Assuming we only planned on making 100000 predictions from get-go, that is all the data we need to calcualte the true generalization error $\mathcal E(f)$ using \ref{gen-error}. Then we will compare our previously estimated generalization error $\hat{\mathcal E}(f)$ to the truth $\mathcal E(f)$ to see if our approach works. We'll repeat the model building process many times while keeping the future data the same so we can see how good our estimates of model performance are on average.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our data factory can be whatever we want, but for this experiment we'll set it up like this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">data_factory</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 1 predictor</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span><span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>

<span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;x&#39;</span><span class="p">:</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span>
<span class="p">})</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">source</span><span class="p">,</span> 
    <span class="n">height</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">(</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_21_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great. Now let's fit a model. I'll use a regression tree for this.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll use absolute error as our evaluation metric and estimate the generalization error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">metric</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">ŷ</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ŷ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ê</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>

<span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;x&#39;</span><span class="p">:</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y</span><span class="p">,</span>
    <span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">ŷ</span>
<span class="p">})</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">source</span><span class="p">,</span> 
    <span class="n">height</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">(</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="p">)</span> <span class="o">+</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">source</span><span class="p">,</span> 
    <span class="n">height</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_27_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we'll generate our "future" data and calculate the real generalization error</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_future</span><span class="p">,</span> <span class="n">y_future</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
<span class="n">ŷ_future</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_future</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_future</span><span class="p">,</span> <span class="n">ŷ_future</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's compare the two:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>ê: 1.3888014843767187
e: 1.9637616259629058
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This doesn't seem promising... our estimate of the generalization error is much smaller than it should be! But maybe we just got unlucky this one time. Let's repeat the simulation many times to make sure.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">simulate_model_eval</span><span class="p">():</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ê</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">ŷ_future</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_future</span><span class="p">)</span> <span class="c1"># we already generated these data</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_future</span><span class="p">,</span> <span class="n">ŷ_future</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ê</span><span class="p">,</span> <span class="n">e</span>

<span class="n">ês</span><span class="p">,</span> <span class="n">es</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">simulate_model_eval</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_34_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ouch. This shows that our estimates are of generalization error are consistently far too low. Out of curiosity, let's see if that still happens if we use a linear model instead of a random forest.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When an estimator is on average less than the quantitiy it is trying to estimate, we say that the estimator is <em>biased downward</em>. Thus $\hat{\mathcal E}$ is biased downward (for $\mathcal{E}$).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>

<span class="k">def</span> <span class="nf">simulate_model_eval</span><span class="p">():</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ê</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ŷ</span><span class="p">)</span>
    
    <span class="n">ŷ_future</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_future</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_future</span><span class="p">,</span> <span class="n">ŷ_future</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ê</span><span class="p">,</span> <span class="n">e</span>

<span class="n">ês</span><span class="p">,</span> <span class="n">es</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">simulate_model_eval</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;estimated generalization error&#39;</span><span class="p">:</span><span class="n">ês</span><span class="p">,</span>
    <span class="s1">&#39;generalization error&#39;</span><span class="p">:</span><span class="n">es</span>
<span class="p">})</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">transform_fold</span><span class="p">(</span>
    <span class="p">[</span><span class="s1">&#39;estimated generalization error&#39;</span><span class="p">,</span> <span class="s1">&#39;generalization error&#39;</span><span class="p">],</span>
    <span class="n">as_</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Quantity&#39;</span><span class="p">,</span> <span class="s1">&#39;error&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;error:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Quantity:N&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_38_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It looks like we still have the same problem, but perhaps to a lesser extent.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Putting all of this together, we have to conclude that we cannot use the estimator defined in \ref{empirical-err}: as far as we can tell based on these experiments, we would always be underestimating the generalization error. Moreover, the reason this happens is <em>not</em> that the past data is not representative of the future data because we are using the exact same <code>data_factory</code> function to generate both datasets. Furthermore, the amount we're off by depends on what kind of model we use (and perhaps other factors we didn't investigate), so we can't even know ahead of time how much correction might be necessary.</p>
<p>This is really bad news for us. If we can't estimate ahead of time how our model will perform, how can we tell which model will be better than another? How can we tell if our model is better than random guessing?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="EXERCISE">EXERCISE<a class="anchor-link" href="#EXERCISE"> </a></h5><p>Repeat these experiments with a <em>classifier</em> of your choosing (e.g. <code>sklearn.ensemble.RandomForestClassifier</code>). You can use this code to generate data with binary outcomes:</p>

<pre><code>def class_data_factory(n):
    X = np.random.random((n,5)) # 5 predictors
    y = np.random.randint(2, size=n)
    return X,y</code></pre>
<p>Use classification accuracy as your evaluation metric. Are your estimates of generalization error still biased downward?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-and-Test-Sets">Training and Test Sets<a class="anchor-link" href="#Training-and-Test-Sets"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although things look bleak at the moment, there is actually a simple solution to our problem. All we have to do is split our data up into two subsamples: one that will be used to fit the model, and one that will be used to evaluate it. The data we use to fit the model is called a <em>training set</em> and the data we use to evaluate the model is called a <em>test set</em>.</p>
<p><img src="https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg" alt=""></p>
<p>In other words, our new estimate of generalization error will be the <em>test error</em>:</p>
$$
\mathcal {\hat E}_{\mathcal T}(f) 
=
\frac{1}{n^{\mathcal T}} \sum_{i \in \mathcal{T}}  M(y_i, f_{\mathcal S}(x_i))
\label{test-error}
$$<p>Where $\mathcal T$ (for "test") and $\mathcal S$ (for "study", since "training" also starts with a T) are a sets of indices indicating which observations are in the test and training sets, respectively. $n^{\mathcal T}$ is the number of observations in the test set. The notation $f_{\mathcal S}$ indicates that the model $f$ was fit using the training data $\mathcal S$ and not the test data.</p>
<p>Compare this to the <em>training error</em>, which is basically what we were using before:</p>
$$
\mathcal {\hat E}_{\mathcal S}(f) 
=
\frac{1}{n^{\mathcal S}} \sum_{i \in \mathcal{S}}  M(y_i, f_{\mathcal S}(x_i))
\label{training-error}
$$<p>We'll demonstrate the use of the test set in code first and then explain why it works.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">simulate_model_eval</span><span class="p">():</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
    
    <span class="n">S</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span> <span class="c1"># training set</span>
    <span class="n">T</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># test set index</span>
    
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">S</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">S</span><span class="p">]</span> <span class="c1"># take the first 25 rows as training data</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">T</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">T</span><span class="p">]</span> <span class="c1"># take the last 25 rows as test data</span>
    <span class="n">ŷ_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">ê</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">ŷ_test</span><span class="p">)</span>
    
    <span class="n">ŷ_future</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_future</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_future</span><span class="p">,</span> <span class="n">ŷ_future</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ê</span><span class="p">,</span> <span class="n">e</span>

<span class="n">ês</span><span class="p">,</span> <span class="n">es</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">simulate_model_eval</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>

<span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;estimated generalization error&#39;</span><span class="p">:</span><span class="n">ês</span><span class="p">,</span>
    <span class="s1">&#39;generalization error&#39;</span><span class="p">:</span><span class="n">es</span>
<span class="p">})</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">transform_fold</span><span class="p">(</span>
    <span class="p">[</span><span class="s1">&#39;estimated generalization error&#39;</span><span class="p">,</span> <span class="s1">&#39;generalization error&#39;</span><span class="p">],</span>
    <span class="n">as_</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Quantity&#39;</span><span class="p">,</span> <span class="s1">&#39;error&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;error:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Quantity:N&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_45_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This looks much, much better. Our estimate of the generalization error isn't perfect- sometimes it's higher than reality and sometimes it's lower, but on average we're getting close.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To figure out why this works we have to think about what the difference is between the test set $(X^{\mathcal T}, y^{\mathcal T})$ and the full observed data $(X, y)$ in terms of how well each of those sets approximate the future data $(X^\dagger, y^\dagger)$. In terms of what the data themselves are, there is clearly no substantive difference between the test set and the full observed data except that the test set is a subset of the observed data. But using a smaller random sample to estimate the generalization error isn't what's fixing the problem.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The real difference is not what's in the data or how much of it there is, but <em>how it is used</em>. The test set is not used to fit the model $f_{\mathcal S}$, whereas the training set is. This matters because <em>future</em> data also cannot ever be used to fit the model since it's data we don't even have at the time the model is fit. In that sense, the test set is a much better approximation of future data than the training data are because, like the future data, the test data are not used to fit the model. The very fact that we have observed the training data and used it to fit the model makes it unsuitable as an approximation to future data, even though we have done nothing to change the data itself!</p>
<p>We're going to dig into this further, but for now it might be useful to think of an analogy. Imagine that a student is taking a course whose ultimate goal is to prepare the student to do well in their future career. The course includes both practice problems (with solutions) and a final exam. How can the student make sure she is prepared to succeed in the future? If the student studies using the practice problems, but then evaluates her performance using those same practice problems, she will always be optimistic about what she has actually learned. She may have simply memorized the answers to those problems or learned certain tricks that only work for those problems. However, her performance on the final exam, which has problems she has never seen before, is more likely to indicate how successful she will be in the future, since she will encounter new problems in her career, not textbook problems.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generalization-Error-Revisited">Generalization Error Revisited<a class="anchor-link" href="#Generalization-Error-Revisited"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We originally presented the generalization error as an average over all future data. That's probably the easiest way to think of it, but it isn't actually the way that we most often work with it mathematically. Mathematically, the generalization error is actually</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\mathcal {E}(f) 
=
E_{X, Y}[ M(Y, f(X)) ]
\label{real-gen-error}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where the expectation is taken over the joint distribution of the random variables $X$ and $Y$. Notice that our fit model $f$ is a fixed function, although $X$ and $Y$ are random.</p>
<p>Now we can see a little more clearly why our training set estimator (equation \ref{training-error}) doesn't work: it's approximating something like $\mathcal {E}_{\mathcal S}(f) = E_{X, Y}[ M(Y, f_{X,Y}(X)) ]
$. We're trying to average over the variation in model performance on the "future" data $X$ and $Y$, but we're using that "future" data to fit the model $f_{X,Y}$ at the same time. In fact, the notation $\mathcal {E}_{\mathcal S}(f)$ doesn't even make sense because $f$ isn't even a fixed function that we're evaluating inside the estimator - it changes along with the data. So what we're evaluating with the theoretical quantity $\mathcal {E}_{\mathcal S}$ isn't even clear.</p>
<p>On the other hand, the test set estimator (equation \ref{test-error}) just replaces the expectation in \ref{real-gen-error} with a finite average, but keeps $f$ fixed when computing that average. Thus, if you take the expected value of the test-set estimator, you get back the generalization error:
$$
\begin{array}{rcl}
E[\hat{\mathcal E}_{\mathcal T}(f)] &amp;=&amp; E\left[ \frac{1}{n^{\mathcal T}} \sum_{i \in \mathcal{T}} M(Y, f_{\mathcal S}(X)) \right] \\
&amp;=&amp;  \frac{1}{n^{\mathcal T}} \sum_{i \in \mathcal{T}} E\left[ M(Y, f_{\mathcal S}(X)) \right] \\
&amp;=&amp; \frac{n^{\mathcal T}}{n^{\mathcal T}} \mathcal E(f)
\end{array}
$$</p>
<p>The math is easy here because the expectation passes through the sum, after which the quantity on the inside of the sum is recognizable as the generalization error. If you try and do the same for the training error, you will end up with a quantity inside the sum that looks like the aforementioned $E_{X, Y}[ M(Y, f_{X,Y}(X))]$. That's usually impossible to work with analytically because you have to factor in how the model gets fit from the data. Point being: the test error is (on average) the generalization error, but it's very hard in general to know how the training error will relate to the generalization error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To demonstrate the difference, we can simulate both training and test errors. Each simulation is like a single draw from the joint distribution of $X$ and $Y$. First we set up the functions that we will call to compute a singe training error and a single test error:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">training_error</span><span class="p">():</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>           <span class="c1"># randomness across different simulation runs comes from here</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>                    <span class="c1"># model depends on the random data</span>
    <span class="n">ŷ_train</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">ŷ_train</span><span class="p">)</span>                <span class="c1"># evaluating a moving (random) target against random data</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>    
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>                     <span class="c1"># model fixed in advance</span>

<span class="k">def</span> <span class="nf">test_error</span><span class="p">():</span>
    <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>          <span class="c1"># randomness across different simulation runs comes from here</span>
    <span class="n">ŷ_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">ŷ_test</span><span class="p">)</span>               <span class="c1"># evaluating a fixed target against random data</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now we repeat over 500 simulations:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ê_S</span><span class="p">,</span> <span class="n">ê_T</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">training_error</span><span class="p">(),</span> <span class="n">test_error</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that the code to fit the model is inside of the <code>training_error</code> function, but outside of the <code>test_error</code> function. Thus the model $f$ will be <em>refit</em> each time we compute a training error, but is treated as <em>fixed</em> when computing each test error. This difference only becomes clear when you do a simulation you can repeat many times, since using a single dataset the model only gets fit once and you don't get to see where the "randomness" enters. When doing simulations, you can see that the randomness enters before the model is even fit when using the training error estimate of generalization error. The randomness only enters after the model has been fit when using the test error estimate of generalization error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we plot the training and test errors, we can see that the training error (which is not an accurate estimate of generalization error) is usually smaller than the test error (which is). In other words, if we use the training error, we will usually overestimate the performance of our model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;training error&#39;</span><span class="p">:</span><span class="n">ê_S</span><span class="p">,</span>
    <span class="s1">&#39;test error&#39;</span><span class="p">:</span><span class="n">ê_T</span>
<span class="p">})</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">transform_fold</span><span class="p">(</span>
    <span class="p">[</span><span class="s1">&#39;training error&#39;</span><span class="p">,</span> <span class="s1">&#39;test error&#39;</span><span class="p">],</span>
    <span class="n">as_</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Quantity&#39;</span><span class="p">,</span> <span class="s1">&#39;estimated metric&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;estimated metric:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Quantity:N&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_60_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Geometry-of-Generalization,-Training,-and-Test-Errors">The Geometry of Generalization, Training, and Test Errors<a class="anchor-link" href="#The-Geometry-of-Generalization,-Training,-and-Test-Errors"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We've demonstrated that training error is a poor estimate of generalization error and specifically that it tends to underestimate the generalization error. But why should training error tend to underestimate instead of overestimate? And why is it that the effect is more pronounced when we use a random forest, but much less pronounced when we use linear regression?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We're going to draw a picture that will help answer these questions and many more. To start, we'll imagine the space of <em>all possible functions</em> of the predictors and we'll call that $\mathbb F$. So, for example, if there is a single predictor $x$, then you can imagine that space is something like $\mathbb F = \{10.2 \dots, x^2, \dots \log(x) + \frac{1}{x}, \dots\}$ so that each element of $\mathbb F$ is some function of $x$ and every possible function of $x$ (of which there are infinite) is contained in $\mathbb F$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The the absolute best possible model we could ever have would be</p>
$$
f^\dagger = \underset{f \in \mathbb F}{\text{argmin}} \ \  \mathcal E (f)
$$<p>In other words, we'll find the function out of all the possible functions in $\mathbb F$ that minimizes the generalization error and call that function $f^\dagger$. By definition, it's not possible to find a better model than that.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can define the "distance" between $f^\dagger$ and any point $f$ as $\mathcal E(f) - \mathcal E(f^\dagger)$. To simplify things, let's assume that $\mathcal E(f^\dagger)$ is 0 so that the generalization error $\mathcal E(f)$ represents the distance between our estimated model $f$ and the best possible model we could have estimated $f^\dagger$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src='gen-err-dist.png' width=400></p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general, there could be multiple functions that simultaneously minimize the generalization error, so $f^\dagger$ might be a set instead of a single point. We'll illustrate it as a point in these figures because it's easier to understand, but the arguments work the same if you draw it as a space instead. In this case, the "distances" are the distance from each point to a boundary of the space.</p>
<p>It's also possible that $\mathcal E(f^\dagger) \ne 0$ but there's no point in carrying around the extra $\mathcal E(f^\dagger)$ everywhere because when we compare distances (say $f_a \leftrightarrow f^\dagger = \mathcal E(f_a) - \mathcal E(f^\dagger)$ vs $f_b \leftrightarrow f^\dagger =  \mathcal E(f_b) - \mathcal E(f^\dagger)$), the $\mathcal E(f^\dagger)$ term drops out (i.e. we get $\mathcal E(f_a) - \mathcal E(f_b))$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's consider fitting a model. We don't know what the best possible model $f^\dagger$ is - for multiple reasons: 1) We can't calculate the expectation over the data-generating distribution because we don't know what that distribution is, and, 2) even if we did, there would be no way to optimize the generalization error over the space of all possible functions.</p>
<p>What we do get to observe are the data $X,y$ in the training set $\mathcal S$. That "solves" the problem of not knowing the data-generating distribution in the sense that we can approximate the expectation over the distribution with an average over the observed data points. That's $\hat {\mathcal E}_{\mathcal S}(f)$. Now imagine we could find the function in $\mathbb F$ that would optimize this training error:</p>
$$
f_{\mathcal S} = \underset{f \in \mathbb F}{\text{argmin}} \ \  \hat{\mathcal E}_{\mathcal S} (f)
$$<p>Unfortunately, we also can't find $f_{\mathcal S}$ because it's not possible (for reasons discussed in previous chapters) to search over the entire space $\mathbb F$. Thus we restrict our search for a model to some smaller space. We call this the model space $\mathcal F$, which is the space of all functions that we are willing to consider for our model $f$. For instance, if we are fitting a cubic model $\hat y_i = a x_i^3 + b x_i^2 + c x_i + d$, we are only considering elements of $\mathbb F$ that have that particular form. So a function like $\log(x) + \frac{1}{x}$ would be in $\mathbb F$, but not in $\mathcal F$. Using a smaller model space is a workaround for the second problem because we are substituting $\mathcal F$ for $\mathbb F$:</p>
$$
f = \underset{f \in \mathcal F}{\text{argmin}} \ \  \hat{\mathcal E}_{\mathcal S} (f)
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And this expression neatly represents the three components of the supervised learning problem: 1) the model space $\mathcal F$, 2) the loss function $\hat{\mathcal E}_{\mathcal S}(f)$, and the optimization algorithm that ends up implementing the $\text{argmin}$ operation. The result is the fit model $f$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We're used to seeing the loss expressed as $L(y, \hat y)$, so how is the training error $\hat{\mathcal E}_{\mathcal S}(f)$ the same as that? Well, if the loss function and the evaluation metric are the same, you'll be able to see how $L(y, \hat y)$ is the same as $\hat{\mathcal E}_{\mathcal S}(f)$ once you write them both out. If the loss and the evaluation metric are <em>not</em> the same, then you can think of the loss as an approximation to the evaluation metric. By finding the model that minimizes the loss, we're hoping to find one that does well in terms of the evaluation metric too.</p>
<p>$\hat{\mathcal E}_{\mathcal S} (f_\mathcal S)$ will actually always be 0 assuming $x_i \ne x_j$ since there is always a function that can perfectly interpolate any finite set of points. Furthermore, there are always an infinite number of interpolating functions, so $f_\mathcal S$ will actually be a set in its own right. As before, we'll draw it as a point to simplify the presentation.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can also define a distance between $f_\mathcal{S}$, the model that theoretically optimizes the training error, and any model $f$ as $\hat{\mathcal E}_{\mathcal S} (f) - \hat{\mathcal E}_{\mathcal S} (f_\mathcal S)$. And, as before, we can assume $\hat{\mathcal E}_{\mathcal S} (f_\mathcal S)=0$ so that the training error $\hat{\mathcal E}_{\mathcal S} (f)$ represents the distance between the estimated model $f$ and the model with the best possible training error $f_\mathcal{S}$.</p>
<p>When we solve our optimization problem to find $f$, we're looking for the model in $\mathcal F$ that minimizes the training error. But since the training error is just the distance between $f$ and $f_{\mathcal S}$, another way of putting this is that is that we're finding the point in $\mathcal F$ that is closest to $f_{\mathcal S}$ (in the hopes that $f_{\mathcal S}$ is close to $f^\dagger$).</p>
<p><img src='tr-gen-error.png' width=300></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this figure we see the relationship between $f^\dagger$ (the best possible model), $f_{\mathcal S}$ (the model with the best possible training error on the observed data) and $f$ (the model we fit). The only one of these that we actually know is $f$, after we fit it. The rest we have to imagine. We can also observe the training error $\hat{\mathcal E}_{\mathcal S}$, which is the distance from $f$ to $f_\mathcal{S}$. But what we really want to know is $\mathcal E$, the distance between $f$ and $f^\dagger$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-Set-Variability">Training Set Variability<a class="anchor-link" href="#Training-Set-Variability"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This figure is drawn assuming a particular training set $\mathcal S$ has been fixed. But the data are random - if we repeated our experiment we would gather slightly different data. $f^\dagger$ stays fixed since it doesn't depend on the training set, but $f_\mathcal{S}$ will move around since it does. That, in turn, means $f$ moves around since it's the point in $\mathcal F$ closest to whatever $f_\mathcal{S}$ happens to be. Consider these possibilities:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table><tr>
    <td><img src=tr-gen-error.png width=200></td>
    <td><img src=tr-gen-error-1.png width=200></td>
</tr></table>
<table><tr>
    <td><img src=tr-gen-error-2.png width=200></td>
    <td><img src=tr-gen-error-3.png width=200></td>
</tr></table>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that almost no matter where $f_\mathcal{S}$ ends up, the training error ends up being less than the generalization error. Sometimes by a lot, sometimes by a little. The reason this happens is that $f$ follows $f_\mathcal{S}$ around as much as possible, which explains why the training error typically underestimates the generalization error. In fact, if there is <em>any</em> point in the model space that is closer to $f_\mathcal{S}$ than to $f^\dagger$, the training error must be smaller than the generalization error because that point (or one with even less training error) will be chosen as $f$. Conversely, the generalization error will only be smaller than the training error if there is no point at all in $\mathcal F$ that is closer to $f_\mathcal{S}$ than it is to $f^\dagger$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, $f_\mathcal{S}$ isn't equally likely to fall anywhere in $\mathbb F$. In fact, it has a probability distribution that is determined by the joint distribution of $X$ and $Y$. That comes from the definition of $f_\mathcal{S}$. Since some data are more likely than others, some models out of all possible models are more likely to fit those data the best.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src='tr-dist.png' width=300></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this picture, the grey shaded areas represent regions that $f_\mathcal{S}$ is more likely to be in. Additionally, as we increase the size of the test set, $f_\mathcal{S}$ should become more and more concentrated around $f^\dagger$ since the training data are becoming a better and better approximation of the entire population that the expectation is being taken over.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><table><tr>
    <td><img src=tr-dist.png width=200></td>
    <td><img src=tr-dist-1.png width=200></td>
    <td><img src=tr-dist-2.png width=200></td>    
    </tr><tr>
</tr></table>
<img src=tr-dist-underset.png width=600></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And, in fact, this is reflected by the average size of the training errors we get in simulations when we vary the size of the training set:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="k">def</span> <span class="nf">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ŷ</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">simulate_model_eval</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="n">n_train</span> <span class="o">+</span> <span class="n">n_test</span><span class="p">)</span>
    
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">n_train</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="n">train_error</span> <span class="o">=</span> <span class="n">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_error</span> <span class="o">=</span> <span class="n">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">gen_error</span> <span class="o">=</span> <span class="n">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_future</span><span class="p">,</span> <span class="n">y_future</span><span class="p">)</span> <span class="c1"># not possible in practice</span>
    
    <span class="k">return</span> <span class="n">train_error</span><span class="p">,</span> <span class="n">test_error</span><span class="p">,</span> <span class="n">gen_error</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="p">,</span> <span class="o">*</span><span class="n">simulate_model_eval</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="p">))</span>
           <span class="k">for</span> <span class="n">n_train</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">n_test</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
                  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">results</span><span class="p">,</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Size of Training Set&#39;</span><span class="p">,</span> <span class="s1">&#39;Size of Test Set&#39;</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;generalization&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
    <span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Size of Training Set&#39;</span><span class="p">,</span> <span class="s1">&#39;Size of Test Set&#39;</span><span class="p">],</span>
    <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;Error Type&#39;</span><span class="p">,</span>
    <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alt</span><span class="o">.</span><span class="n">data_transformers</span><span class="o">.</span><span class="n">disable_max_rows</span><span class="p">()</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">results</span><span class="p">[</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;Size of Test Set&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">100</span><span class="p">],</span> 
    <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">400</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;Error:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Error Type:N&#39;</span><span class="p">),</span>
    <span class="n">row</span><span class="o">=</span><span class="s1">&#39;Size of Training Set:N&#39;</span>
<span class="c1">#     column=&#39;Size of Test Set:N&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">resolve_scale</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_85_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice how the training errors vary more wildly but are usualy smaller when training set is small. That happens because $f_\mathcal{S}$ is bouncing around a much larger area and thus has a bigger chance of coming close to $\mathcal F$, as illustrated in the figure above. As the size of the training set increases, the likely "location" of $f_\mathcal{S}$ converges around where $f$ is, so the training errors get larger (since $f$ is presumably somehwere away from $\mathcal F$), but also more consistent in size and closer to the size of the generalization error.</p>
<p>The other trend in these plots is that the test error (and generalization error, which it approximates) are shrinking as the size of the training data increases. That also makes sense: the model is better when we train it using more data. Why? Because $f_\mathcal{S}$ gets closer to $f^\dagger$ with more training data, and since $f_\mathcal{S}$ pulls $f$ along, $f$ gets closer to $f^\dagger$ too (as much as it can within the confines of $\mathcal F$).</p>
<p>You can also see this if you simulate small training sets and visualize the fit models. It's much easier for the model to pass through (interpolate) all of the data points when there are fewer data points. Since the predictions are very close to the data points, the error is small. As the amount of training data increases, the model has a harder time interpolating all of the points, so the training error increases.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sim_train</span><span class="p">(</span><span class="n">n_train</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="n">n_train</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

    <span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;x&#39;</span><span class="p">:</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
        <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y_train</span><span class="p">,</span>
        <span class="s1">&#39;ŷ&#39;</span><span class="p">:</span><span class="n">ŷ</span>
    <span class="p">})</span>

    <span class="n">chart</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
        <span class="n">source</span><span class="p">,</span> 
        <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mi">300</span>
    <span class="p">)</span><span class="o">.</span><span class="n">mark_point</span><span class="p">(</span>
    <span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
        <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">+</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
        <span class="n">source</span><span class="p">,</span> 
        <span class="n">height</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span>
    <span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
        <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;ŷ&#39;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">chart</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alt</span><span class="o">.</span><span class="n">vconcat</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">sim_train</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span> <span class="o">|</span> <span class="n">alt</span><span class="o">.</span><span class="n">vconcat</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">sim_train</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_88_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another way of thinking about this phenomenon is that it's very easy for the model to simply "memorize" the training data when there is very little of it. It will always perform well on the training data because it has the answers memorized, but as every student knows, memorizing homework problems won't help too much when the test comes along. As the size of the training data increase, the model struggles to memorize each individual case and must instead capture a generalizable pattern to do well on the training data, which is what improves the performance on the test data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Test-Set-Variability">Test Set Variability<a class="anchor-link" href="#Test-Set-Variability"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although the test error seems to approximate the generalization error fairly well, the approximation isn't perfect. That's because the test set itself is also a random subset of the the data. Like we did for the training set, we can define the model $f_{\mathcal T}$ out of all possible models $\mathbb F$ that would best fit the test set:</p>
$$
f_{\mathcal T} = \underset{f \in \mathbb F}{\text{argmin}} \ \  \hat{\mathcal E}_{\mathcal T} (f)
$$<p>and again we can define the distance between any other function $f$ and $f_\mathcal{T}$ as $\hat{\mathcal E}_{\mathcal T} (f)$. So the distance from a function $f$ to the best possible model on the test data $f_\mathcal{T}$ is precisely the test error of that function.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src='te-tr-gen-error.png' width=300></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$f_\mathcal{T}$ will also move around depending on what the test data are:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table><tr>
    <td><img src=te-tr-gen-error.png width=200></td>
    <td><img src=te-tr-gen-error-1.png width=200></td>
    <td><img src=te-tr-gen-error-2.png width=200></td>
    </tr><tr>
</tr></table>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And, like $f_\mathcal{S}$, it will be more likely to fall in regions around $f^\dagger$. In fact, since the test set is a random sample from the same data-generating distribution as the training sample, the likely locations of $f_\mathcal{S}$ will be the exact same as that of $f_\mathcal{T}$, although "shrunk" or "expanded" depending on the relative sizes of the training and test sets.</p>
<p>Both $f_\mathcal{S}$ and $f_\mathcal{T}$ are likely to be near $f^\dagger$. Either one is just as likely to be closer to or further from $\mathcal F$ than the other. The difference is that $f_\mathcal{S}$ will determine exactly where in $\mathcal F$ the fit model $f$ will end up, whereas $f_\mathcal{T}$ has no impact on $f$. Again, that's why the test error is a fair estimate of the generalization error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That geometrical intiution is once again borne out by simulations. Let's see what happens when we hold the size of the training data constant, but increase the size of the test data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alt</span><span class="o">.</span><span class="n">data_transformers</span><span class="o">.</span><span class="n">disable_max_rows</span><span class="p">()</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">results</span><span class="p">[</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;Size of Training Set&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">100</span><span class="p">],</span> 
    <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">400</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;Error:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Error Type:N&#39;</span><span class="p">),</span>
    <span class="n">row</span><span class="o">=</span><span class="s1">&#39;Size of Test Set:N&#39;</span>
<span class="c1">#     column=&#39;Size of Test Set:N&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">resolve_scale</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_98_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In every case, the test error is a fair estimate of the generalization error, in that it's not higher or lower on average. However, as we increase the size of the test set, the test error becomes a better and better estimate of the generalization error. That's because $f_\mathcal{T}$ is getting pulled closer and closer to $f^\dagger$, so the distance $\hat{\mathcal E}_\mathcal{S}$ becomes a better approximation of the distance $\mathcal E$.</p>
<p>However, the overall quality of these models (as captured by the generalization errors) is not changing. That also makes sense: the size of the test data doesn't impact the model we get because the model is trained using only the training data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="EXERCISE">EXERCISE<a class="anchor-link" href="#EXERCISE"> </a></h5><p>So far we've only considered cases where $f^\dagger$ is outside of $\mathcal F$. In practice, this will always be the case. Real-world data are just too complex to be perfectly captured inside of any model space that we can come up with, no matter how complex. But massive deep learning models are so big and flexible that every point in $\mathbb F$ is near to some point in $\mathcal F$ (more on this later), so it's educational to consider the case where $f^\dagger \in \mathcal F$.</p>
<p>Let's say our data are generated by this data-generating process:</p>

<pre><code>def data_factory(n):
    X = np.random.random((n,1)) 
    y = -2*X[:,0] + 3
    return X,y</code></pre>
<p>and our evaluation metric is mean-squared error: $(y-\hat y)^2$.</p>
<p>Draw a sample of 10 data points from this data-generating process and plot $y$ vs $x$.</p>
<p>What is the best possible model for these data? i.e., what is $f^\dagger(x)$? (<em>hint</em>: you don't have to prove anything- intuition suffices)</p>
<p>If we let $\mathcal F$ be the space of linear models (i.e. we're doing linear regression), is $f^\dagger(x) \in \mathcal F$? What does that mean about the best-possible generalization error that we could acheive using a linear model? Draw a picture including $\mathbb F$, $\mathcal F$ and $f^\dagger$.</p>
<p>Let's say we generate a training and test set from this data-generating process, and we use the training set to fit a linear model and the test set to evaluate it. What is the training error we will get? What will the test error be? Why? Add examples of where $f_\mathcal{S}$ and $f_\mathcal{T}$ might be to your picture. (<em>hint</em>: if you're not sure, implement the code, generate the data, fit the model, evaluate, and see what happens!)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_popout">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="EXERCISE">EXERCISE<a class="anchor-link" href="#EXERCISE"> </a></h5><p>Let's say our data are generated by this data-generating process:</p>

<pre><code>def data_factory(n):
    X = np.random.random((n,1)) 
    y = -2*X[:,0] + 3 + np.random.normal(size=(n,1))
    return X,y</code></pre>
<p>and our evaluation metric is mean-squared error: $(y-\hat y)^2$.</p>
<p>Draw a sample of 10 data points from this data-generating process and plot $y$ vs $x$.</p>
<p>What is the best possible model for these data? i.e., what is $f^\dagger(x)$?</p>
<p>If we let $\mathcal F$ be the space of linear models (i.e. we're doing linear regression), is $f^\dagger(x) \in \mathcal F$? What does that mean about the best-possible generalization error that we could acheive using a linear model? Draw a picture including $\mathbb F$, $\mathcal F$ and $f^\dagger$.</p>
<p>Let's say we generate a training and test set from this data-generating process, and we use the training set to fit a linear model and the test set to evaluate it. Will we get a training error of 0? What about the test error? Why? Add examples of where $f_\mathcal{S}$ and $f_\mathcal{T}$ might be to your picture. (<em>hint</em>: if you're not sure, implement the code, generate the data, fit the model, evaluate, and see what happens!)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Trade-Offs-and-Cross-Testing">Trade-Offs and Cross-Testing<a class="anchor-link" href="#Trade-Offs-and-Cross-Testing"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We've been looking at the effects of the size of the training and test sets on training, test, and generalization error. But, in practice, we can't vary those two sizes independently of one another. We collect a single dataset and must split it into training and test sets.</p>
<p>That induces a tradeoff: if we want a model that's likely to have low generalization error, we need more training data. But having more training data means less test data, so our estimate of model performance will be much more variable! On the other hand, if we want to have a very accurate estimate of how well our model performs, we need more test data. But having more test data means less training data, so our model will be worse than it otherwise could have been!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="p">,</span> <span class="o">*</span><span class="n">simulate_model_eval</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="p">))</span>
           <span class="k">for</span> <span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="p">)</span> <span class="ow">in</span> 
           <span class="p">[(</span><span class="mi">10</span><span class="p">,</span><span class="mi">990</span><span class="p">),</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">900</span><span class="p">),</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span><span class="mi">500</span><span class="p">),</span> <span class="p">(</span><span class="mi">900</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span> <span class="p">(</span><span class="mi">990</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
               <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">results</span><span class="p">,</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Size of Training Set&#39;</span><span class="p">,</span> <span class="s1">&#39;Size of Test Set&#39;</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;generalization&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
    <span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Size of Training Set&#39;</span><span class="p">,</span> <span class="s1">&#39;Size of Test Set&#39;</span><span class="p">],</span>
    <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;Error Type&#39;</span><span class="p">,</span>
    <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">Split</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">df</span><span class="p">:</span> <span class="p">[</span><span class="n">f</span><span class="s1">&#39;Train:</span><span class="si">{s}</span><span class="s1">, Test:</span><span class="si">{t}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">t</span> <span class="ow">in</span> 
                        <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Size of Training Set&quot;</span><span class="p">],</span> 
                            <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Size of Test Set&quot;</span><span class="p">])]</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alt</span><span class="o">.</span><span class="n">data_transformers</span><span class="o">.</span><span class="n">disable_max_rows</span><span class="p">()</span>

<span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">results</span><span class="p">,</span>
<span class="c1">#     results[results[&#39;Size of Test Set&#39;]==100], </span>
    <span class="n">height</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">400</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;Error:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Error Type:N&#39;</span><span class="p">),</span>
    <span class="n">row</span><span class="o">=</span><span class="s1">&#39;Split:N&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">resolve_scale</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_106_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The tradeoff is clearly visible here. At the top, we get a poorly performing model (high and variable generalization error) but the observed test error lines up with the unobserved generalization error almost perfectly. At the bottom we have a model that performs well (lower generalization error with almost no variability), but the observed test error is all over the place and thus is not a good indicator of the unobserved generalization error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In practice, the choice of how to split your data is up to you. Would you rather have a better model, or a better estimate of how good that model is?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, things aren't quite as bleak as they seem because after a certain point more training data stops increasing the performance by any substantial amount. That's visible in the plots above: the generalization errors of the models trained on 500 data points look about as good as the generalization errors of the models trained using 1000 data points.</p>
<p>In other words, model performance "asymptotes" after a certain amount of training data. This happens because $f_\mathcal{S}$ converges to $f^\dagger$ at some finite rate as the size of the training data increase. After a certain point, $f_\mathcal{S}$ will already be very close to $f^\dagger$ and the additional distance between the two will be small. Closing that distance by further increasing the size of the training set therefore yields smaller and smaller improvements in performance.</p>
<p>Unfortunately, it's also not possible to know exactly the point at which this happens because we can't directly measure the generalization error. But in the world of "big data" with millions of datapoints you can usually safely assume that your model would be equally good if trained on a fraction of the data. So, while there are no hard and fast rules, you can use a smaller proportion of training data if your dataset is large, but it's safer to use a larger proportion of training data if your dataset is small.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Cross-Testing">Cross-Testing<a class="anchor-link" href="#Cross-Testing"> </a></h4><p>One solution to the problem presented by this tradeoff is <em>cross-testing</em> (also called <em>cross-validation</em>, but we'll reserve that term for later use when we discuss model selection). In cross-testing, the data are evenly split up into $k$ (say, 5) folds. We take one of the folds (#1) and use it as test data, training our model on the other $k-1$ folds. We then repeat the process, holding out each of the other $k-1$ folds in turn. At the end we will have fit $k$ models and calculated $k$ test errors, one for each fold.</p>
<p>Each of those test errors is likely to be somewhat variable, since each individual test set may be relatively small. The clever thing about cross-testing, however, is that we can now <em>average</em> all of these errors together to get a final estimate of the generalization error (call this the <em>cross-test error</em>) that will be less variable than each of the $k$ test errors. Let's demonstrate:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_output">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">KFold</span>

<span class="k">def</span> <span class="nf">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">ŷ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ŷ</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cross_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">test_errors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">S</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">S</span><span class="p">])</span>
        <span class="n">test_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">T</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">T</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">test_errors</span>

<span class="k">def</span> <span class="nf">simulate_model_eval</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data_factory</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="n">test_errors</span> <span class="o">=</span> <span class="n">cross_test</span><span class="p">(</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">cross_test_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_errors</span><span class="p">)</span>
    <span class="n">gen_error</span> <span class="o">=</span> <span class="n">model_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_future</span><span class="p">,</span> <span class="n">y_future</span><span class="p">)</span> <span class="c1"># not possible in practice</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="o">*</span><span class="n">test_errors</span><span class="p">,</span> <span class="n">cross_test_error</span><span class="p">,</span> <span class="n">gen_error</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">simulate_model_eval</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">results</span><span class="p">,</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">,</span><span class="s1">&#39;2&#39;</span><span class="p">,</span><span class="s1">&#39;3&#39;</span><span class="p">,</span><span class="s1">&#39;4&#39;</span><span class="p">,</span><span class="s1">&#39;cross-test&#39;</span><span class="p">,</span> <span class="s1">&#39;generalization&#39;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
    <span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cross-test&#39;</span><span class="p">,</span> <span class="s1">&#39;generalization&#39;</span><span class="p">],</span>
    <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;Fold&#39;</span><span class="p">,</span>
    <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;test (fold)&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
    <span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Fold&#39;</span><span class="p">],</span>
    <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;Error Type&#39;</span><span class="p">,</span>
    <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span>
    <span class="n">results</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">200</span>
<span class="p">)</span><span class="o">.</span><span class="n">mark_area</span><span class="p">(</span>
    <span class="n">opacity</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;step&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;Error:Q&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Bin</span><span class="p">(</span><span class="n">maxbins</span><span class="o">=</span><span class="mi">100</span><span class="p">)),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;count()&#39;</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;Error Type:N&#39;</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">facet</span><span class="p">(</span>
    <span class="n">facet</span><span class="o">=</span><span class="s1">&#39;Fold:N&#39;</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span><span class="o">.</span><span class="n">resolve_scale</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;independent&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/model-eval/model-eval_114_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, the cross-test errors are more concentrated around the true generalization error than the test errors using any of the folds. In effect, we've gotten more confidence that our test error will be near the generalization error without having to dedicate more data to the test set. The only cost is computation: we had to fit $k$ models instead of just one.</p>
<p>If we fit $k$ models, which of them do we use? The most common practice is to fit a final model using all of the observed data as training data. This final model itself will not be evaluated, but presumably its performance will only be equal or better than the average performance of the five models that were each trained on only $(k-1)/k$-ths of the total data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Changing-the-Model-Space">Changing the Model Space<a class="anchor-link" href="#Changing-the-Model-Space"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Besides the relative sizes of the training and test sets, we also have control over one very important thing: the model space we choose to use!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>smaller/bigger model spaces: The size and shape of $\mathcal F$ is what determines how much the model can memorize</li>
<li>bias/variance</li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    