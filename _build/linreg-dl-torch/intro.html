---
interact_link: content/linreg-dl-torch/intro.ipynb
kernel_name: python3
kernel_path: content/linreg-dl-torch
has_widgets: false
title: |-
  From Linear Regression to Deep Learning in Pytorch
pagenum: 5
prev_page:
  url: /grad-descent/grad-descent.html
next_page:
  url: /linreg-dl-torch/linear-reg.html
suffix: .ipynb
search: models regression learning gradient descent linear deep pytorch chapter well build supervised seemingly arbitrary complexity without having manually specify particular parametric form key stack top outputs previous become inputs subsequent instead training simultaneously search combined parameter spaces layers deriving expressions gradients loss relative parameters incredibly tedious introduce auto differentiation tool provided torch pacakge breeze implement

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">From Linear Regression to Deep Learning in Pytorch</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this chapter we'll see how to build supervised learning models of seemingly arbitrary complexity without having to manually specify a particular parametric form. The key to this is to stack regression models on top of each other so that the outputs of previous models become the inputs to the subsequent models. But instead of training these models up one-at-a-time, we can use gradient descent to simultaneously search the combined parameter spaces of each of these "layers". Deriving the expressions for the gradients of the loss relative to each of these parameters would be incredibly tedious, so we will introduce an auto-differentiation tool provided by the <code>torch</code> pacakge that will make gradient descent a breeze to implement.</p>

</div>
</div>
</div>
</div>

 


    </main>
    