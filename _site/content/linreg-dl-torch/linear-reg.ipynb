{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a simple linear regression using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll create some random data and store it as torch tensors. For now you can think of a torch tensor as equivalent to a numpy array, but we will soon see that tensors have some additional functionality that we can exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 5\n",
    "x = torch.randn(n, p) # predictors (100 observations, 5 features)\n",
    "y = torch.randn(n, 1) # outcomes (100 observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for parameters (coefficients) $\\beta$ so that \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "y_i & \\approx & \\beta_0 + x_i\\beta_{1:p}  \\\\\n",
    "&= &\\beta_0 + x_{i1}\\beta_1 +x_{i2}\\beta_2 \\dots x_{ip}\\beta_p\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you may not have seen it represented this way before, we can also write this model as a picture:\n",
    "\n",
    "<img src=\"images/linreg.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will come in handy later when we get to more complex models, but for now you can think of the linear model in whatever way seems natural to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To quantify what we mean by a \"good\" approximation, we'll use the mean-squared-error loss. So for a given guess $\\beta$ we'll give it the \"grade\" \n",
    "\n",
    "$$L(y,\\hat y) = \\frac{1}{n}\\sum_i (y_i - \\hat y_i)^2$$\n",
    "\n",
    "where $\\hat y_i  = x_i\\beta_{1:p} + \\beta_0$ and $n$ is the number of observations (rows) in the data. We're looking for the $\\beta$ that gives us the best (lowest) grade. This combination of model (linear) and loss (mean-squared-error) is called linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are an infinite number of possible values for $\\beta$ that we could try, so it would take us forever to try them all and see which gives the best loss. To get around this problem, we need some kind of optimization algorithm that is better than brute-force search. The algorithm we will use here is an extremely useful approach called gradient descent, which you should already be familiar with from our [previous exploration]().\n",
    "\n",
    "To start, we'll initialize the coefficients $\\beta$ with random numbers. That's our first guess. We'll update these random numbers using gradient descent to iteratively find better values that make the loss smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2731],\n",
       "        [-0.1205],\n",
       "        [-0.1281],\n",
       "        [ 1.1380],\n",
       "        [ 1.7139],\n",
       "        [ 1.7325]], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β = torch.randn(p+1, 1) # 5 coefficients (one per feature) plus one intercept\n",
    "β.requires_grad_() # tell torch that β is going to have to save the gradient of something with respect to itself at some point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously when using gradient descent we would have to analytically derive expressions for the gradient of the loss relative to each model parameter, implement these as functions in code, and call upon them at each gradient descent iteration. With pytorch, however, we can simply compute the current value of the loss and pytorch will automatically calculate all the necessary derivatives for us. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = torch.matmul(x, β[1:]) + β[0] # ŷ = xβ + β0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = torch.sum((y-ŷ)**2)/n # L = Σ(yᵢ-ŷᵢ)²/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.5604, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward() # compute the gradients of the loss with respect to any tensors that went into the loss with requires_grad=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4644],\n",
       "        [-0.3877],\n",
       "        [ 0.4751],\n",
       "        [ 1.6801],\n",
       "        [ 3.0645],\n",
       "        [ 5.0251]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we see here is a vector containing all of the derivatives we want. The first element is $\\frac{\\delta L}{\\delta \\beta_0}$, the second element is $\\frac{\\delta L}{\\delta \\beta_1}$, and so on. Note that this object is part of $\\beta$ and not $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the gradient manually and make sure it matches up. We have:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hat y_i &=& x_i\\beta + \\beta_0 \\\\\n",
    "L(y,\\hat y) &=& \\frac{1}{n}\\sum_i (y_i - \\hat y_i)^2\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the derivative for $\\beta_j$ with $j\\ne 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\frac{\\partial L}{\\partial \\beta_j} &=& \n",
    "\\frac{1}{n}\n",
    "\\sum_i \n",
    "\\frac{\\partial L}{\\partial y_i} \n",
    "\\frac{\\partial y_i}{\\partial \\beta_j}\n",
    "\\\\\n",
    "&=&\n",
    "\\frac{1}{n}\n",
    "\\sum_i\n",
    "-2(y_i-\\hat y_i)\n",
    "x_{ij}\n",
    "\\\\\n",
    "&=&\n",
    "-\\frac{2}{n}\n",
    "x_j^T(y-\\hat y)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and for $\\beta_0$ is \n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\frac{\\partial L}{\\partial \\beta_0}\n",
    "&=&\n",
    "\\frac{1}{n}\n",
    "\\sum_i\n",
    "-2(y_i-\\hat y_i) \\\\\n",
    "&=&\n",
    "-\\frac{2}{n}\n",
    "1^T(y-\\hat y)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "which means we can calculate the whole gradient as $-\\frac{2}{n}[1,x]^T(y-[1,x]\\beta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def beta_grad(β):\n",
    "    x_with_1s = torch.Tensor(np.hstack((np.ones((n,1)), x)))\n",
    "    return -2*torch.matmul(x_with_1s.transpose(0,1), y-torch.matmul(x_with_1s,β))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4644],\n",
       "        [-0.3877],\n",
       "        [ 0.4751],\n",
       "        [ 1.6801],\n",
       "        [ 3.0645],\n",
       "        [ 5.0251]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_grad(β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is exactly the same as the result in `β.grad`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does pytorch do this? It turns out that every pytorch tensor records not only its own value, but also what functions were called to produce it and which tensors went into those functions. That's why we use torch tensors instead of numpy arrays. The `torchvis` package lets us see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"204pt\" height=\"432pt\"\n",
       " viewBox=\"0.00 0.00 204.31 432.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 428)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-428 200.31,-428 200.31,4 -4,4\"/>\n",
       "<!-- 4928321744 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4928321744</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"139.47,-20 51.18,-20 51.18,0 139.47,0 139.47,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">DivBackward0</text>\n",
       "</g>\n",
       "<!-- 4928321424 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4928321424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"141.48,-76 49.17,-76 49.17,-56 141.48,-56 141.48,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 4928321424&#45;&gt;4928321744 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4928321424&#45;&gt;4928321744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.32,-55.59C95.32,-48.7 95.32,-39.1 95.32,-30.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.82,-30.3 95.32,-20.3 91.82,-30.3 98.82,-30.3\"/>\n",
       "</g>\n",
       "<!-- 4928321168 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4928321168</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"141.31,-132 49.34,-132 49.34,-112 141.31,-112 141.31,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 4928321168&#45;&gt;4928321424 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4928321168&#45;&gt;4928321424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.32,-111.59C95.32,-104.7 95.32,-95.1 95.32,-86.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.82,-86.3 95.32,-76.3 91.82,-86.3 98.82,-86.3\"/>\n",
       "</g>\n",
       "<!-- 4928320272 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4928320272</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"140.14,-188 50.51,-188 50.51,-168 140.14,-168 140.14,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 4928320272&#45;&gt;4928321168 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4928320272&#45;&gt;4928321168</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.32,-167.59C95.32,-160.7 95.32,-151.1 95.32,-142.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.82,-142.3 95.32,-132.3 91.82,-142.3 98.82,-142.3\"/>\n",
       "</g>\n",
       "<!-- 4928321296 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4928321296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"141.14,-244 49.51,-244 49.51,-224 141.14,-224 141.14,-244\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 4928321296&#45;&gt;4928320272 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4928321296&#45;&gt;4928320272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.32,-223.59C95.32,-216.7 95.32,-207.1 95.32,-198.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.82,-198.3 95.32,-188.3 91.82,-198.3 98.82,-198.3\"/>\n",
       "</g>\n",
       "<!-- 4928319888 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4928319888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"84.47,-300 0.18,-300 0.18,-280 84.47,-280 84.47,-300\"/>\n",
       "<text text-anchor=\"middle\" x=\"42.32\" y=\"-286.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 4928319888&#45;&gt;4928321296 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4928319888&#45;&gt;4928321296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.56,-279.59C59.16,-271.85 70.11,-260.69 79.16,-251.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.69,-253.89 86.2,-244.3 76.7,-248.98 81.69,-253.89\"/>\n",
       "</g>\n",
       "<!-- 4928323472 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4928323472</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"103.46,-356 15.18,-356 15.18,-336 103.46,-336 103.46,-356\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.32\" y=\"-342.4\" font-family=\"Times,serif\" font-size=\"12.00\">SliceBackward</text>\n",
       "</g>\n",
       "<!-- 4928323472&#45;&gt;4928319888 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4928323472&#45;&gt;4928319888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M56.36,-335.59C54.14,-328.55 51.04,-318.67 48.31,-310\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"51.59,-308.79 45.25,-300.3 44.92,-310.89 51.59,-308.79\"/>\n",
       "</g>\n",
       "<!-- 4928322064 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4928322064</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"122.32,-424 68.32,-424 68.32,-392 122.32,-392 122.32,-424\"/>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-410.4\" font-family=\"Times,serif\" font-size=\"12.00\">β</text>\n",
       "<text text-anchor=\"middle\" x=\"95.32\" y=\"-398.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (6, 1)</text>\n",
       "</g>\n",
       "<!-- 4928322064&#45;&gt;4928323472 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4928322064&#45;&gt;4928323472</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.24,-391.86C81.34,-383.69 75.25,-373.55 70.11,-364.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.96,-362.93 64.82,-356.15 66.96,-366.53 72.96,-362.93\"/>\n",
       "</g>\n",
       "<!-- 4928320720 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>4928320720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196.29,-300 102.36,-300 102.36,-280 196.29,-280 196.29,-300\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.32\" y=\"-286.4\" font-family=\"Times,serif\" font-size=\"12.00\">SelectBackward</text>\n",
       "</g>\n",
       "<!-- 4928322064&#45;&gt;4928320720 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4928322064&#45;&gt;4928320720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.38,-391.84C112.25,-370.64 130.15,-332.2 140.76,-309.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.02,-310.68 145.07,-300.14 137.68,-307.72 144.02,-310.68\"/>\n",
       "</g>\n",
       "<!-- 4928320720&#45;&gt;4928321296 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4928320720&#45;&gt;4928321296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.92,-279.59C132.17,-271.85 121.01,-260.69 111.79,-251.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"114.17,-248.89 104.62,-244.3 109.22,-253.84 114.17,-248.89\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x125c03090>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(L, {'β':β})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradient of $L$ with respect to $\\beta$, pytorch takes the gradients of each of these functions in turn (which are simple and hardcoded into pytorch), evaluates them at their current value using the input tensors, and multiplies them together to arrive at the answer you would get via the chain rule. You can learn more about this process [here](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95) and elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights, we need to subtract the gradient (times a small learning rate) from the current value of the weights. \n",
    "\n",
    "We do this from inside a `no_grad():` \"context\" so that $\\beta$ doesn't store the history of the update (try the update without the `with torch.no_grad():` and see what happens). We also clear out `β.grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    β -= 10e-5 * β.grad # β = β - 10e-5 * β.grad\n",
    "    β.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2731],\n",
       "        [-0.1204],\n",
       "        [-0.1282],\n",
       "        [ 1.1378],\n",
       "        [ 1.7136],\n",
       "        [ 1.7320]], requires_grad=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our new value of $\\beta$ is slightly different than what we started with because we've taken a single gradient step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "popout"
    ]
   },
   "source": [
    "**ZEROING GRADIENTS**\n",
    "\n",
    "If we don't zero the gradient, the next time we calculate the gradient of something with respect to $\\beta$, the new gradient will be added to whatever was stored there instead of overwriting it. That's just the way torch was made to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "popout"
    ]
   },
   "source": [
    "---\n",
    "**EXERCISE**\n",
    "\n",
    "Investigate for yourself (either by testing code or googling) why the parameter update and gradient zerioing should be performed within a `torch.no_grad()` context. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat what we have so far but now add a little loop to train our model for 500 gradient descent iterations instead of going slowly though a single iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1092],\n",
       "        [-1.3379],\n",
       "        [ 0.2449],\n",
       "        [-0.8324],\n",
       "        [-0.3387],\n",
       "        [ 0.4786]], requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(n, p) # predictors (10 observations, 5 features)\n",
    "y = torch.randn(n, 1) # outcomes (10 observations)\n",
    "\n",
    "loss_record = [] # to kep track of the loss over the iterations\n",
    "β = torch.randn(p+1,1) # 5 coefficients (one per feature) plus one intercept\n",
    "β.requires_grad_() # tell torch that β is going to have to save the gradient of something with respect to itself at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(500):\n",
    "    ŷ = torch.matmul(x, β[1:]) + β[0] # ŷ = xβ + β0 (calculate predictions)\n",
    "    L = torch.sum((y-ŷ)**2)/n # L = Σ(yᵢ-ŷᵢ)²/n (use predictions to calculate loss)\n",
    "    \n",
    "    L.backward() # compute gradients (in this case δL/δβ, δL/δW)\n",
    "    loss_record.append(L.item())\n",
    "    \n",
    "    with torch.no_grad(): # take the gradient descent step \n",
    "        β -= 10e-3 * β.grad\n",
    "        β.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see how the loss changes over the iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v3+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v3.4.0.json",
       "config": {
        "mark": {
         "tooltip": null
        },
        "view": {
         "height": 300,
         "width": 400
        }
       },
       "data": {
        "name": "data-8bbf64d689001292b93509c84ba11f81"
       },
       "datasets": {
        "data-8bbf64d689001292b93509c84ba11f81": [
         {
          "i": 0,
          "loss": 3.0417187213897705
         },
         {
          "i": 1,
          "loss": 2.9595117568969727
         },
         {
          "i": 2,
          "loss": 2.8805413246154785
         },
         {
          "i": 3,
          "loss": 2.804675579071045
         },
         {
          "i": 4,
          "loss": 2.731790065765381
         },
         {
          "i": 5,
          "loss": 2.6617631912231445
         },
         {
          "i": 6,
          "loss": 2.5944814682006836
         },
         {
          "i": 7,
          "loss": 2.5298328399658203
         },
         {
          "i": 8,
          "loss": 2.467712163925171
         },
         {
          "i": 9,
          "loss": 2.408017158508301
         },
         {
          "i": 10,
          "loss": 2.350651741027832
         },
         {
          "i": 11,
          "loss": 2.295520305633545
         },
         {
          "i": 12,
          "loss": 2.242534637451172
         },
         {
          "i": 13,
          "loss": 2.191608428955078
         },
         {
          "i": 14,
          "loss": 2.1426589488983154
         },
         {
          "i": 15,
          "loss": 2.095607280731201
         },
         {
          "i": 16,
          "loss": 2.050377607345581
         },
         {
          "i": 17,
          "loss": 2.0068962574005127
         },
         {
          "i": 18,
          "loss": 1.965095043182373
         },
         {
          "i": 19,
          "loss": 1.9249058961868286
         },
         {
          "i": 20,
          "loss": 1.8862648010253906
         },
         {
          "i": 21,
          "loss": 1.8491101264953613
         },
         {
          "i": 22,
          "loss": 1.8133829832077026
         },
         {
          "i": 23,
          "loss": 1.7790263891220093
         },
         {
          "i": 24,
          "loss": 1.745985984802246
         },
         {
          "i": 25,
          "loss": 1.7142095565795898
         },
         {
          "i": 26,
          "loss": 1.6836471557617188
         },
         {
          "i": 27,
          "loss": 1.654250979423523
         },
         {
          "i": 28,
          "loss": 1.6259747743606567
         },
         {
          "i": 29,
          "loss": 1.5987738370895386
         },
         {
          "i": 30,
          "loss": 1.5726062059402466
         },
         {
          "i": 31,
          "loss": 1.5474308729171753
         },
         {
          "i": 32,
          "loss": 1.5232092142105103
         },
         {
          "i": 33,
          "loss": 1.4999034404754639
         },
         {
          "i": 34,
          "loss": 1.4774775505065918
         },
         {
          "i": 35,
          "loss": 1.4558968544006348
         },
         {
          "i": 36,
          "loss": 1.4351284503936768
         },
         {
          "i": 37,
          "loss": 1.4151408672332764
         },
         {
          "i": 38,
          "loss": 1.3959029912948608
         },
         {
          "i": 39,
          "loss": 1.3773856163024902
         },
         {
          "i": 40,
          "loss": 1.3595608472824097
         },
         {
          "i": 41,
          "loss": 1.3424017429351807
         },
         {
          "i": 42,
          "loss": 1.3258819580078125
         },
         {
          "i": 43,
          "loss": 1.3099772930145264
         },
         {
          "i": 44,
          "loss": 1.294663429260254
         },
         {
          "i": 45,
          "loss": 1.2799171209335327
         },
         {
          "i": 46,
          "loss": 1.2657170295715332
         },
         {
          "i": 47,
          "loss": 1.2520418167114258
         },
         {
          "i": 48,
          "loss": 1.2388710975646973
         },
         {
          "i": 49,
          "loss": 1.2261853218078613
         },
         {
          "i": 50,
          "loss": 1.213965892791748
         },
         {
          "i": 51,
          "loss": 1.2021952867507935
         },
         {
          "i": 52,
          "loss": 1.1908555030822754
         },
         {
          "i": 53,
          "loss": 1.1799304485321045
         },
         {
          "i": 54,
          "loss": 1.1694037914276123
         },
         {
          "i": 55,
          "loss": 1.1592609882354736
         },
         {
          "i": 56,
          "loss": 1.1494868993759155
         },
         {
          "i": 57,
          "loss": 1.140067458152771
         },
         {
          "i": 58,
          "loss": 1.1309889554977417
         },
         {
          "i": 59,
          "loss": 1.122239112854004
         },
         {
          "i": 60,
          "loss": 1.113804817199707
         },
         {
          "i": 61,
          "loss": 1.1056742668151855
         },
         {
          "i": 62,
          "loss": 1.0978360176086426
         },
         {
          "i": 63,
          "loss": 1.0902787446975708
         },
         {
          "i": 64,
          "loss": 1.0829918384552002
         },
         {
          "i": 65,
          "loss": 1.0759657621383667
         },
         {
          "i": 66,
          "loss": 1.0691896677017212
         },
         {
          "i": 67,
          "loss": 1.062654972076416
         },
         {
          "i": 68,
          "loss": 1.056352138519287
         },
         {
          "i": 69,
          "loss": 1.0502727031707764
         },
         {
          "i": 70,
          "loss": 1.0444084405899048
         },
         {
          "i": 71,
          "loss": 1.0387506484985352
         },
         {
          "i": 72,
          "loss": 1.0332924127578735
         },
         {
          "i": 73,
          "loss": 1.0280258655548096
         },
         {
          "i": 74,
          "loss": 1.0229439735412598
         },
         {
          "i": 75,
          "loss": 1.0180398225784302
         },
         {
          "i": 76,
          "loss": 1.0133068561553955
         },
         {
          "i": 77,
          "loss": 1.0087387561798096
         },
         {
          "i": 78,
          "loss": 1.0043294429779053
         },
         {
          "i": 79,
          "loss": 1.000072956085205
         },
         {
          "i": 80,
          "loss": 0.9959638118743896
         },
         {
          "i": 81,
          "loss": 0.9919965267181396
         },
         {
          "i": 82,
          "loss": 0.9881659746170044
         },
         {
          "i": 83,
          "loss": 0.9844669103622437
         },
         {
          "i": 84,
          "loss": 0.980894923210144
         },
         {
          "i": 85,
          "loss": 0.9774448871612549
         },
         {
          "i": 86,
          "loss": 0.9741126298904419
         },
         {
          "i": 87,
          "loss": 0.970893919467926
         },
         {
          "i": 88,
          "loss": 0.9677846431732178
         },
         {
          "i": 89,
          "loss": 0.9647808074951172
         },
         {
          "i": 90,
          "loss": 0.9618783593177795
         },
         {
          "i": 91,
          "loss": 0.9590737819671631
         },
         {
          "i": 92,
          "loss": 0.9563637375831604
         },
         {
          "i": 93,
          "loss": 0.9537445902824402
         },
         {
          "i": 94,
          "loss": 0.9512133598327637
         },
         {
          "i": 95,
          "loss": 0.9487664699554443
         },
         {
          "i": 96,
          "loss": 0.9464013576507568
         },
         {
          "i": 97,
          "loss": 0.9441148638725281
         },
         {
          "i": 98,
          "loss": 0.9419043660163879
         },
         {
          "i": 99,
          "loss": 0.9397667050361633
         },
         {
          "i": 100,
          "loss": 0.9376997947692871
         },
         {
          "i": 101,
          "loss": 0.9357007741928101
         },
         {
          "i": 102,
          "loss": 0.9337676167488098
         },
         {
          "i": 103,
          "loss": 0.9318979382514954
         },
         {
          "i": 104,
          "loss": 0.9300892353057861
         },
         {
          "i": 105,
          "loss": 0.9283394813537598
         },
         {
          "i": 106,
          "loss": 0.9266467094421387
         },
         {
          "i": 107,
          "loss": 0.9250089526176453
         },
         {
          "i": 108,
          "loss": 0.9234241247177124
         },
         {
          "i": 109,
          "loss": 0.9218904972076416
         },
         {
          "i": 110,
          "loss": 0.9204061031341553
         },
         {
          "i": 111,
          "loss": 0.9189696311950684
         },
         {
          "i": 112,
          "loss": 0.917579174041748
         },
         {
          "i": 113,
          "loss": 0.9162331223487854
         },
         {
          "i": 114,
          "loss": 0.9149301052093506
         },
         {
          "i": 115,
          "loss": 0.9136683940887451
         },
         {
          "i": 116,
          "loss": 0.9124466776847839
         },
         {
          "i": 117,
          "loss": 0.911263644695282
         },
         {
          "i": 118,
          "loss": 0.9101179242134094
         },
         {
          "i": 119,
          "loss": 0.9090085625648499
         },
         {
          "i": 120,
          "loss": 0.9079338908195496
         },
         {
          "i": 121,
          "loss": 0.9068927764892578
         },
         {
          "i": 122,
          "loss": 0.9058845043182373
         },
         {
          "i": 123,
          "loss": 0.9049074053764343
         },
         {
          "i": 124,
          "loss": 0.9039608240127563
         },
         {
          "i": 125,
          "loss": 0.9030434489250183
         },
         {
          "i": 126,
          "loss": 0.9021544456481934
         },
         {
          "i": 127,
          "loss": 0.901293158531189
         },
         {
          "i": 128,
          "loss": 0.9004581570625305
         },
         {
          "i": 129,
          "loss": 0.899648904800415
         },
         {
          "i": 130,
          "loss": 0.8988644480705261
         },
         {
          "i": 131,
          "loss": 0.8981040120124817
         },
         {
          "i": 132,
          "loss": 0.8973665833473206
         },
         {
          "i": 133,
          "loss": 0.8966516852378845
         },
         {
          "i": 134,
          "loss": 0.8959584832191467
         },
         {
          "i": 135,
          "loss": 0.8952862024307251
         },
         {
          "i": 136,
          "loss": 0.8946341872215271
         },
         {
          "i": 137,
          "loss": 0.8940019011497498
         },
         {
          "i": 138,
          "loss": 0.8933884501457214
         },
         {
          "i": 139,
          "loss": 0.8927934765815735
         },
         {
          "i": 140,
          "loss": 0.8922163248062134
         },
         {
          "i": 141,
          "loss": 0.8916565179824829
         },
         {
          "i": 142,
          "loss": 0.8911129832267761
         },
         {
          "i": 143,
          "loss": 0.8905858397483826
         },
         {
          "i": 144,
          "loss": 0.8900741338729858
         },
         {
          "i": 145,
          "loss": 0.8895778656005859
         },
         {
          "i": 146,
          "loss": 0.8890959024429321
         },
         {
          "i": 147,
          "loss": 0.8886282444000244
         },
         {
          "i": 148,
          "loss": 0.8881742358207703
         },
         {
          "i": 149,
          "loss": 0.887733519077301
         },
         {
          "i": 150,
          "loss": 0.8873056173324585
         },
         {
          "i": 151,
          "loss": 0.8868902325630188
         },
         {
          "i": 152,
          "loss": 0.8864868879318237
         },
         {
          "i": 153,
          "loss": 0.8860952258110046
         },
         {
          "i": 154,
          "loss": 0.8857149481773376
         },
         {
          "i": 155,
          "loss": 0.8853455185890198
         },
         {
          "i": 156,
          "loss": 0.8849868774414062
         },
         {
          "i": 157,
          "loss": 0.8846383690834045
         },
         {
          "i": 158,
          "loss": 0.8842998743057251
         },
         {
          "i": 159,
          "loss": 0.8839711546897888
         },
         {
          "i": 160,
          "loss": 0.8836517333984375
         },
         {
          "i": 161,
          "loss": 0.883341372013092
         },
         {
          "i": 162,
          "loss": 0.8830399513244629
         },
         {
          "i": 163,
          "loss": 0.8827468752861023
         },
         {
          "i": 164,
          "loss": 0.8824621438980103
         },
         {
          "i": 165,
          "loss": 0.8821855187416077
         },
         {
          "i": 166,
          "loss": 0.8819166421890259
         },
         {
          "i": 167,
          "loss": 0.8816553354263306
         },
         {
          "i": 168,
          "loss": 0.8814013004302979
         },
         {
          "i": 169,
          "loss": 0.8811544179916382
         },
         {
          "i": 170,
          "loss": 0.8809143900871277
         },
         {
          "i": 171,
          "loss": 0.8806809782981873
         },
         {
          "i": 172,
          "loss": 0.8804542422294617
         },
         {
          "i": 173,
          "loss": 0.8802337050437927
         },
         {
          "i": 174,
          "loss": 0.8800192475318909
         },
         {
          "i": 175,
          "loss": 0.8798105716705322
         },
         {
          "i": 176,
          "loss": 0.8796076774597168
         },
         {
          "i": 177,
          "loss": 0.8794105052947998
         },
         {
          "i": 178,
          "loss": 0.8792187571525574
         },
         {
          "i": 179,
          "loss": 0.8790320754051208
         },
         {
          "i": 180,
          "loss": 0.8788506388664246
         },
         {
          "i": 181,
          "loss": 0.8786742687225342
         },
         {
          "i": 182,
          "loss": 0.8785024285316467
         },
         {
          "i": 183,
          "loss": 0.8783354163169861
         },
         {
          "i": 184,
          "loss": 0.8781729340553284
         },
         {
          "i": 185,
          "loss": 0.8780148029327393
         },
         {
          "i": 186,
          "loss": 0.8778610229492188
         },
         {
          "i": 187,
          "loss": 0.8777113556861877
         },
         {
          "i": 188,
          "loss": 0.8775656819343567
         },
         {
          "i": 189,
          "loss": 0.877423882484436
         },
         {
          "i": 190,
          "loss": 0.8772860169410706
         },
         {
          "i": 191,
          "loss": 0.8771517872810364
         },
         {
          "i": 192,
          "loss": 0.8770210146903992
         },
         {
          "i": 193,
          "loss": 0.876893937587738
         },
         {
          "i": 194,
          "loss": 0.8767700791358948
         },
         {
          "i": 195,
          "loss": 0.8766496777534485
         },
         {
          "i": 196,
          "loss": 0.8765321969985962
         },
         {
          "i": 197,
          "loss": 0.8764181733131409
         },
         {
          "i": 198,
          "loss": 0.8763068914413452
         },
         {
          "i": 199,
          "loss": 0.8761987090110779
         },
         {
          "i": 200,
          "loss": 0.8760933876037598
         },
         {
          "i": 201,
          "loss": 0.8759906888008118
         },
         {
          "i": 202,
          "loss": 0.8758907914161682
         },
         {
          "i": 203,
          "loss": 0.8757935166358948
         },
         {
          "i": 204,
          "loss": 0.8756988644599915
         },
         {
          "i": 205,
          "loss": 0.8756065368652344
         },
         {
          "i": 206,
          "loss": 0.8755165934562683
         },
         {
          "i": 207,
          "loss": 0.8754292130470276
         },
         {
          "i": 208,
          "loss": 0.8753440380096436
         },
         {
          "i": 209,
          "loss": 0.8752608299255371
         },
         {
          "i": 210,
          "loss": 0.8751801252365112
         },
         {
          "i": 211,
          "loss": 0.8751013875007629
         },
         {
          "i": 212,
          "loss": 0.8750246167182922
         },
         {
          "i": 213,
          "loss": 0.8749498724937439
         },
         {
          "i": 214,
          "loss": 0.8748770952224731
         },
         {
          "i": 215,
          "loss": 0.8748061656951904
         },
         {
          "i": 216,
          "loss": 0.874737024307251
         },
         {
          "i": 217,
          "loss": 0.8746697306632996
         },
         {
          "i": 218,
          "loss": 0.8746041059494019
         },
         {
          "i": 219,
          "loss": 0.8745401501655579
         },
         {
          "i": 220,
          "loss": 0.8744778633117676
         },
         {
          "i": 221,
          "loss": 0.874417245388031
         },
         {
          "i": 222,
          "loss": 0.874358057975769
         },
         {
          "i": 223,
          "loss": 0.8743003606796265
         },
         {
          "i": 224,
          "loss": 0.874244213104248
         },
         {
          "i": 225,
          "loss": 0.874189555644989
         },
         {
          "i": 226,
          "loss": 0.8741362690925598
         },
         {
          "i": 227,
          "loss": 0.8740841746330261
         },
         {
          "i": 228,
          "loss": 0.8740335702896118
         },
         {
          "i": 229,
          "loss": 0.873984158039093
         },
         {
          "i": 230,
          "loss": 0.8739359974861145
         },
         {
          "i": 231,
          "loss": 0.8738890290260315
         },
         {
          "i": 232,
          "loss": 0.8738433718681335
         },
         {
          "i": 233,
          "loss": 0.8737987279891968
         },
         {
          "i": 234,
          "loss": 0.8737552165985107
         },
         {
          "i": 235,
          "loss": 0.8737128973007202
         },
         {
          "i": 236,
          "loss": 0.8736716508865356
         },
         {
          "i": 237,
          "loss": 0.8736313581466675
         },
         {
          "i": 238,
          "loss": 0.8735920786857605
         },
         {
          "i": 239,
          "loss": 0.8735538721084595
         },
         {
          "i": 240,
          "loss": 0.8735165596008301
         },
         {
          "i": 241,
          "loss": 0.8734800815582275
         },
         {
          "i": 242,
          "loss": 0.8734446167945862
         },
         {
          "i": 243,
          "loss": 0.8734100461006165
         },
         {
          "i": 244,
          "loss": 0.8733763098716736
         },
         {
          "i": 245,
          "loss": 0.8733433485031128
         },
         {
          "i": 246,
          "loss": 0.8733112812042236
         },
         {
          "i": 247,
          "loss": 0.8732800483703613
         },
         {
          "i": 248,
          "loss": 0.8732495307922363
         },
         {
          "i": 249,
          "loss": 0.8732197284698486
         },
         {
          "i": 250,
          "loss": 0.873190701007843
         },
         {
          "i": 251,
          "loss": 0.8731625080108643
         },
         {
          "i": 252,
          "loss": 0.8731348514556885
         },
         {
          "i": 253,
          "loss": 0.8731079697608948
         },
         {
          "i": 254,
          "loss": 0.8730815649032593
         },
         {
          "i": 255,
          "loss": 0.8730561137199402
         },
         {
          "i": 256,
          "loss": 0.8730310201644897
         },
         {
          "i": 257,
          "loss": 0.8730068206787109
         },
         {
          "i": 258,
          "loss": 0.8729830980300903
         },
         {
          "i": 259,
          "loss": 0.8729599118232727
         },
         {
          "i": 260,
          "loss": 0.8729372620582581
         },
         {
          "i": 261,
          "loss": 0.8729150891304016
         },
         {
          "i": 262,
          "loss": 0.8728936910629272
         },
         {
          "i": 263,
          "loss": 0.8728728294372559
         },
         {
          "i": 264,
          "loss": 0.8728521466255188
         },
         {
          "i": 265,
          "loss": 0.8728322386741638
         },
         {
          "i": 266,
          "loss": 0.872812807559967
         },
         {
          "i": 267,
          "loss": 0.8727939128875732
         },
         {
          "i": 268,
          "loss": 0.8727754354476929
         },
         {
          "i": 269,
          "loss": 0.8727572560310364
         },
         {
          "i": 270,
          "loss": 0.8727394938468933
         },
         {
          "i": 271,
          "loss": 0.8727223873138428
         },
         {
          "i": 272,
          "loss": 0.8727054595947266
         },
         {
          "i": 273,
          "loss": 0.8726891279220581
         },
         {
          "i": 274,
          "loss": 0.8726732134819031
         },
         {
          "i": 275,
          "loss": 0.8726577162742615
         },
         {
          "i": 276,
          "loss": 0.8726423382759094
         },
         {
          "i": 277,
          "loss": 0.8726274967193604
         },
         {
          "i": 278,
          "loss": 0.8726130723953247
         },
         {
          "i": 279,
          "loss": 0.8725988268852234
         },
         {
          "i": 280,
          "loss": 0.8725852370262146
         },
         {
          "i": 281,
          "loss": 0.872571587562561
         },
         {
          "i": 282,
          "loss": 0.8725585341453552
         },
         {
          "i": 283,
          "loss": 0.8725457787513733
         },
         {
          "i": 284,
          "loss": 0.8725330829620361
         },
         {
          "i": 285,
          "loss": 0.8725210428237915
         },
         {
          "i": 286,
          "loss": 0.8725090622901917
         },
         {
          "i": 287,
          "loss": 0.8724973797798157
         },
         {
          "i": 288,
          "loss": 0.8724861145019531
         },
         {
          "i": 289,
          "loss": 0.8724750280380249
         },
         {
          "i": 290,
          "loss": 0.8724642992019653
         },
         {
          "i": 291,
          "loss": 0.8724536299705505
         },
         {
          "i": 292,
          "loss": 0.8724433779716492
         },
         {
          "i": 293,
          "loss": 0.8724333047866821
         },
         {
          "i": 294,
          "loss": 0.8724236488342285
         },
         {
          "i": 295,
          "loss": 0.8724139332771301
         },
         {
          "i": 296,
          "loss": 0.8724046945571899
         },
         {
          "i": 297,
          "loss": 0.8723956346511841
         },
         {
          "i": 298,
          "loss": 0.872386634349823
         },
         {
          "i": 299,
          "loss": 0.8723779320716858
         },
         {
          "i": 300,
          "loss": 0.8723695874214172
         },
         {
          "i": 301,
          "loss": 0.8723613023757935
         },
         {
          "i": 302,
          "loss": 0.8723531365394592
         },
         {
          "i": 303,
          "loss": 0.8723452687263489
         },
         {
          "i": 304,
          "loss": 0.8723375797271729
         },
         {
          "i": 305,
          "loss": 0.8723300695419312
         },
         {
          "i": 306,
          "loss": 0.8723227977752686
         },
         {
          "i": 307,
          "loss": 0.8723155856132507
         },
         {
          "i": 308,
          "loss": 0.8723085522651672
         },
         {
          "i": 309,
          "loss": 0.8723019361495972
         },
         {
          "i": 310,
          "loss": 0.8722952008247375
         },
         {
          "i": 311,
          "loss": 0.8722886443138123
         },
         {
          "i": 312,
          "loss": 0.8722823262214661
         },
         {
          "i": 313,
          "loss": 0.872276246547699
         },
         {
          "i": 314,
          "loss": 0.8722702264785767
         },
         {
          "i": 315,
          "loss": 0.8722643256187439
         },
         {
          "i": 316,
          "loss": 0.8722584247589111
         },
         {
          "i": 317,
          "loss": 0.872252881526947
         },
         {
          "i": 318,
          "loss": 0.8722473978996277
         },
         {
          "i": 319,
          "loss": 0.8722421526908875
         },
         {
          "i": 320,
          "loss": 0.8722368478775024
         },
         {
          "i": 321,
          "loss": 0.8722318410873413
         },
         {
          "i": 322,
          "loss": 0.8722267746925354
         },
         {
          "i": 323,
          "loss": 0.8722220063209534
         },
         {
          "i": 324,
          "loss": 0.8722172379493713
         },
         {
          "i": 325,
          "loss": 0.8722125291824341
         },
         {
          "i": 326,
          "loss": 0.8722079992294312
         },
         {
          "i": 327,
          "loss": 0.8722035884857178
         },
         {
          "i": 328,
          "loss": 0.872199296951294
         },
         {
          "i": 329,
          "loss": 0.8721950650215149
         },
         {
          "i": 330,
          "loss": 0.8721909523010254
         },
         {
          "i": 331,
          "loss": 0.8721869587898254
         },
         {
          "i": 332,
          "loss": 0.8721831440925598
         },
         {
          "i": 333,
          "loss": 0.8721792697906494
         },
         {
          "i": 334,
          "loss": 0.8721754550933838
         },
         {
          "i": 335,
          "loss": 0.8721718788146973
         },
         {
          "i": 336,
          "loss": 0.8721683621406555
         },
         {
          "i": 337,
          "loss": 0.8721648454666138
         },
         {
          "i": 338,
          "loss": 0.8721613883972168
         },
         {
          "i": 339,
          "loss": 0.8721580505371094
         },
         {
          "i": 340,
          "loss": 0.872154951095581
         },
         {
          "i": 341,
          "loss": 0.8721518516540527
         },
         {
          "i": 342,
          "loss": 0.8721485733985901
         },
         {
          "i": 343,
          "loss": 0.8721455931663513
         },
         {
          "i": 344,
          "loss": 0.8721427321434021
         },
         {
          "i": 345,
          "loss": 0.8721397519111633
         },
         {
          "i": 346,
          "loss": 0.8721370697021484
         },
         {
          "i": 347,
          "loss": 0.8721343874931335
         },
         {
          "i": 348,
          "loss": 0.8721316456794739
         },
         {
          "i": 349,
          "loss": 0.8721290826797485
         },
         {
          "i": 350,
          "loss": 0.8721264600753784
         },
         {
          "i": 351,
          "loss": 0.8721240162849426
         },
         {
          "i": 352,
          "loss": 0.8721215724945068
         },
         {
          "i": 353,
          "loss": 0.8721193075180054
         },
         {
          "i": 354,
          "loss": 0.8721169233322144
         },
         {
          "i": 355,
          "loss": 0.8721147775650024
         },
         {
          "i": 356,
          "loss": 0.872112512588501
         },
         {
          "i": 357,
          "loss": 0.8721103668212891
         },
         {
          "i": 358,
          "loss": 0.8721082210540771
         },
         {
          "i": 359,
          "loss": 0.8721063137054443
         },
         {
          "i": 360,
          "loss": 0.8721041679382324
         },
         {
          "i": 361,
          "loss": 0.8721020221710205
         },
         {
          "i": 362,
          "loss": 0.872100293636322
         },
         {
          "i": 363,
          "loss": 0.8720985651016235
         },
         {
          "i": 364,
          "loss": 0.8720965385437012
         },
         {
          "i": 365,
          "loss": 0.8720948100090027
         },
         {
          "i": 366,
          "loss": 0.8720930218696594
         },
         {
          "i": 367,
          "loss": 0.8720913529396057
         },
         {
          "i": 368,
          "loss": 0.872089684009552
         },
         {
          "i": 369,
          "loss": 0.8720880746841431
         },
         {
          "i": 370,
          "loss": 0.8720865845680237
         },
         {
          "i": 371,
          "loss": 0.8720850348472595
         },
         {
          "i": 372,
          "loss": 0.8720836043357849
         },
         {
          "i": 373,
          "loss": 0.8720820546150208
         },
         {
          "i": 374,
          "loss": 0.8720807433128357
         },
         {
          "i": 375,
          "loss": 0.8720792531967163
         },
         {
          "i": 376,
          "loss": 0.8720777630805969
         },
         {
          "i": 377,
          "loss": 0.8720765709877014
         },
         {
          "i": 378,
          "loss": 0.8720751404762268
         },
         {
          "i": 379,
          "loss": 0.8720738887786865
         },
         {
          "i": 380,
          "loss": 0.872072696685791
         },
         {
          "i": 381,
          "loss": 0.8720713257789612
         },
         {
          "i": 382,
          "loss": 0.8720703125
         },
         {
          "i": 383,
          "loss": 0.8720691800117493
         },
         {
          "i": 384,
          "loss": 0.8720681667327881
         },
         {
          "i": 385,
          "loss": 0.872066855430603
         },
         {
          "i": 386,
          "loss": 0.8720657825469971
         },
         {
          "i": 387,
          "loss": 0.8720647692680359
         },
         {
          "i": 388,
          "loss": 0.8720635771751404
         },
         {
          "i": 389,
          "loss": 0.872062623500824
         },
         {
          "i": 390,
          "loss": 0.8720617890357971
         },
         {
          "i": 391,
          "loss": 0.8720607161521912
         },
         {
          "i": 392,
          "loss": 0.8720599412918091
         },
         {
          "i": 393,
          "loss": 0.8720589280128479
         },
         {
          "i": 394,
          "loss": 0.8720578551292419
         },
         {
          "i": 395,
          "loss": 0.8720571994781494
         },
         {
          "i": 396,
          "loss": 0.872056245803833
         },
         {
          "i": 397,
          "loss": 0.8720554113388062
         },
         {
          "i": 398,
          "loss": 0.8720546960830688
         },
         {
          "i": 399,
          "loss": 0.8720539212226868
         },
         {
          "i": 400,
          "loss": 0.8720531463623047
         },
         {
          "i": 401,
          "loss": 0.8720522522926331
         },
         {
          "i": 402,
          "loss": 0.8720515966415405
         },
         {
          "i": 403,
          "loss": 0.8720508813858032
         },
         {
          "i": 404,
          "loss": 0.8720501065254211
         },
         {
          "i": 405,
          "loss": 0.8720495104789734
         },
         {
          "i": 406,
          "loss": 0.8720489740371704
         },
         {
          "i": 407,
          "loss": 0.8720481991767883
         },
         {
          "i": 408,
          "loss": 0.872047483921051
         },
         {
          "i": 409,
          "loss": 0.8720468878746033
         },
         {
          "i": 410,
          "loss": 0.8720462918281555
         },
         {
          "i": 411,
          "loss": 0.8720457553863525
         },
         {
          "i": 412,
          "loss": 0.8720450401306152
         },
         {
          "i": 413,
          "loss": 0.8720446228981018
         },
         {
          "i": 414,
          "loss": 0.8720439672470093
         },
         {
          "i": 415,
          "loss": 0.8720433115959167
         },
         {
          "i": 416,
          "loss": 0.8720428943634033
         },
         {
          "i": 417,
          "loss": 0.8720424175262451
         },
         {
          "i": 418,
          "loss": 0.8720417618751526
         },
         {
          "i": 419,
          "loss": 0.8720414042472839
         },
         {
          "i": 420,
          "loss": 0.8720409274101257
         },
         {
          "i": 421,
          "loss": 0.8720403909683228
         },
         {
          "i": 422,
          "loss": 0.8720399737358093
         },
         {
          "i": 423,
          "loss": 0.8720394968986511
         },
         {
          "i": 424,
          "loss": 0.8720390796661377
         },
         {
          "i": 425,
          "loss": 0.8720386624336243
         },
         {
          "i": 426,
          "loss": 0.8720382452011108
         },
         {
          "i": 427,
          "loss": 0.8720378875732422
         },
         {
          "i": 428,
          "loss": 0.872037410736084
         },
         {
          "i": 429,
          "loss": 0.8720370531082153
         },
         {
          "i": 430,
          "loss": 0.8720366954803467
         },
         {
          "i": 431,
          "loss": 0.8720362782478333
         },
         {
          "i": 432,
          "loss": 0.872035801410675
         },
         {
          "i": 433,
          "loss": 0.8720355033874512
         },
         {
          "i": 434,
          "loss": 0.8720352053642273
         },
         {
          "i": 435,
          "loss": 0.8720349073410034
         },
         {
          "i": 436,
          "loss": 0.8720346093177795
         },
         {
          "i": 437,
          "loss": 0.8720341324806213
         },
         {
          "i": 438,
          "loss": 0.8720338940620422
         },
         {
          "i": 439,
          "loss": 0.8720335960388184
         },
         {
          "i": 440,
          "loss": 0.8720332384109497
         },
         {
          "i": 441,
          "loss": 0.8720329999923706
         },
         {
          "i": 442,
          "loss": 0.872032642364502
         },
         {
          "i": 443,
          "loss": 0.8720324039459229
         },
         {
          "i": 444,
          "loss": 0.8720321655273438
         },
         {
          "i": 445,
          "loss": 0.8720319271087646
         },
         {
          "i": 446,
          "loss": 0.8720316290855408
         },
         {
          "i": 447,
          "loss": 0.8720314502716064
         },
         {
          "i": 448,
          "loss": 0.8720311522483826
         },
         {
          "i": 449,
          "loss": 0.8720309734344482
         },
         {
          "i": 450,
          "loss": 0.8720306158065796
         },
         {
          "i": 451,
          "loss": 0.8720304369926453
         },
         {
          "i": 452,
          "loss": 0.8720301985740662
         },
         {
          "i": 453,
          "loss": 0.8720299601554871
         },
         {
          "i": 454,
          "loss": 0.872029721736908
         },
         {
          "i": 455,
          "loss": 0.8720295429229736
         },
         {
          "i": 456,
          "loss": 0.8720294237136841
         },
         {
          "i": 457,
          "loss": 0.872029185295105
         },
         {
          "i": 458,
          "loss": 0.8720288872718811
         },
         {
          "i": 459,
          "loss": 0.8720288276672363
         },
         {
          "i": 460,
          "loss": 0.8720285892486572
         },
         {
          "i": 461,
          "loss": 0.8720284104347229
         },
         {
          "i": 462,
          "loss": 0.8720281720161438
         },
         {
          "i": 463,
          "loss": 0.8720280528068542
         },
         {
          "i": 464,
          "loss": 0.8720278739929199
         },
         {
          "i": 465,
          "loss": 0.872027575969696
         },
         {
          "i": 466,
          "loss": 0.8720275163650513
         },
         {
          "i": 467,
          "loss": 0.8720274567604065
         },
         {
          "i": 468,
          "loss": 0.8720270395278931
         },
         {
          "i": 469,
          "loss": 0.8720271587371826
         },
         {
          "i": 470,
          "loss": 0.8720269203186035
         },
         {
          "i": 471,
          "loss": 0.8720267415046692
         },
         {
          "i": 472,
          "loss": 0.8720266819000244
         },
         {
          "i": 473,
          "loss": 0.8720264434814453
         },
         {
          "i": 474,
          "loss": 0.8720263838768005
         },
         {
          "i": 475,
          "loss": 0.8720262050628662
         },
         {
          "i": 476,
          "loss": 0.8720261454582214
         },
         {
          "i": 477,
          "loss": 0.8720259070396423
         },
         {
          "i": 478,
          "loss": 0.8720258474349976
         },
         {
          "i": 479,
          "loss": 0.872025728225708
         },
         {
          "i": 480,
          "loss": 0.872025728225708
         },
         {
          "i": 481,
          "loss": 0.8720256090164185
         },
         {
          "i": 482,
          "loss": 0.8720254302024841
         },
         {
          "i": 483,
          "loss": 0.8720253109931946
         },
         {
          "i": 484,
          "loss": 0.8720250725746155
         },
         {
          "i": 485,
          "loss": 0.8720251321792603
         },
         {
          "i": 486,
          "loss": 0.8720250129699707
         },
         {
          "i": 487,
          "loss": 0.8720248341560364
         },
         {
          "i": 488,
          "loss": 0.8720248341560364
         },
         {
          "i": 489,
          "loss": 0.8720247745513916
         },
         {
          "i": 490,
          "loss": 0.8720245361328125
         },
         {
          "i": 491,
          "loss": 0.8720245361328125
         },
         {
          "i": 492,
          "loss": 0.8720244765281677
         },
         {
          "i": 493,
          "loss": 0.8720242977142334
         },
         {
          "i": 494,
          "loss": 0.8720242381095886
         },
         {
          "i": 495,
          "loss": 0.8720240592956543
         },
         {
          "i": 496,
          "loss": 0.8720240592956543
         },
         {
          "i": 497,
          "loss": 0.8720239996910095
         },
         {
          "i": 498,
          "loss": 0.8720239400863647
         },
         {
          "i": 499,
          "loss": 0.8720238208770752
         }
        ]
       },
       "encoding": {
        "x": {
         "field": "i",
         "type": "quantitative"
        },
        "y": {
         "field": "loss",
         "type": "quantitative"
        }
       },
       "height": 100,
       "mark": "line"
      },
      "image/png": "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",
      "text/plain": [
       "<VegaLite 3 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame({'i':range(500), 'loss':loss_record})\n",
    "alt.Chart(loss_df, height=100).mark_line().encode(x='i', y='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see training loss goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the value of $\\beta$ after 500 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0952],\n",
       "        [-0.1218],\n",
       "        [-0.1502],\n",
       "        [-0.1120],\n",
       "        [-0.0219],\n",
       "        [ 0.1727]], requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our original data is totally random and there is no relationship between the predictors and outcomes. So the \"right\" answer for what $\\beta$ should be in this case is $\\beta=0$. As we see above, all the values are near 0, so our algorithm appears to be converging to the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict for a new observation, all we have to do is multiply by $\\beta$ and add the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1722],\n",
       "        [ 0.3556],\n",
       "        [-0.0930],\n",
       "        [-0.0107],\n",
       "        [-0.1134],\n",
       "        [ 0.1378],\n",
       "        [ 0.4761],\n",
       "        [-0.3901],\n",
       "        [ 0.2622],\n",
       "        [-0.6157]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x = torch.randn(10, 5) # 10 new observations\n",
    "torch.matmul(new_x, β[1:]) + β[0] # predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just developed a machine learning method out of these three components:\n",
    "* linear model (model)\n",
    "* MSE loss (loss)\n",
    "* gradient descent (search algorithm)\n",
    "\n",
    "These components are like interchangable parts. We're going to see how we can use a different model and loss to fit data of a different kind without fundamentally changing our search strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
